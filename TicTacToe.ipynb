{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfef69e8-4fac-4f07-b04e-54bae6846e3c",
   "metadata": {},
   "source": [
    "### Решать задачу Крестики-Нолики будем с использованием одного из табличных алгоритмов SARSA. \n",
    "#### Создадим вспомогательный класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a991f596-8297-4211-9d6e-2294bb6625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e42a0dbd-da87-4a6f-b789-217d566b9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunctionClass:\n",
    "    def __init__(self):\n",
    "        self.qdict = {}\n",
    "        self.action_n = 9\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        k = str(k)\n",
    "        if k in self.qdict:\n",
    "            return self.qdict[k]\n",
    "        else:\n",
    "            self.qdict[k] = np.zeros(self.action_n)\n",
    "            return self.qdict[k]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        k = str(k)\n",
    "        self.qdict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b82f27f-46f3-4b72-b065-fb4c5ca4f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_action(self, obsevation):\n",
    "        mask = observation[\"action_mask\"]\n",
    "        action = env.action_space('player_2').sample(mask)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9e016-aea4-4af1-80d5-fd79d65276ed",
   "metadata": {},
   "source": [
    "#### Создаем агента работающего по алгоритму SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "554f69fd-d521-4a5b-8ef9-914fa55e8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    def __init__(self, action_dim=9, side=\"X\", alpha=0.5, episode_n=1000):\n",
    "        self.qfunction = QFunctionClass()\n",
    "        self.action_dim = action_dim\n",
    "        self.side = side\n",
    "        self.prev_state = ''\n",
    "        self.prev_action = -1\n",
    "        self.prev_reward = -1\n",
    "        self.state = ''\n",
    "        self.action = -1\n",
    "        self.reward = -1\n",
    "        self.episode_n = episode_n\n",
    "        self.episode = 0.0\n",
    "        self.epsilon = 1.0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def get_epsilon_greedy_action(self, q_values, epsilon, action_n, mask):\n",
    "        policy = np.ones(action_n) * epsilon / action_n\n",
    "        masked_q_values = [-1.0 if mask[i] == 0 else q_values[i] for i in range(len(q_values))]\n",
    "        max_action = np.argmax(masked_q_values)\n",
    "        policy[max_action] += 1 - epsilon\n",
    "        masked_policy = policy * mask\n",
    "        p = 1.0 - np.sum(masked_policy)\n",
    "        masked_policy = masked_policy + p / (1 - p) * masked_policy\n",
    "        return np.random.choice(np.arange(action_n), p=masked_policy)\n",
    "\n",
    "    def fit(self, reward, done):\n",
    "        self.prev_reward = self.reward\n",
    "        self.reward = reward\n",
    "        if done:\n",
    "            self.qfunction[self.state][self.action] += self.alpha * (self.reward - self.gamma * self.qfunction[self.state][self.action])\n",
    "        else:\n",
    "            if self.prev_state != '':\n",
    "                self.qfunction[self.prev_state][self.prev_action] +=\\\n",
    "                self.alpha * (self.reward + self.gamma * self.qfunction[self.state][self.action]\\\n",
    "                              - self.qfunction[self.prev_state][self.prev_action])\n",
    "        self.episode += 1.0\n",
    "        self.epsilon = max(1.0 - self.episode / self.episode_n, 1e-6)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        mask = observation[\"action_mask\"]               \n",
    "        obs = observation[\"observation\"]\n",
    "        flatten_obs = np.array(obs).reshape(-1)\n",
    "        flatten_obs = [str(i) for i in flatten_obs]\n",
    "        # закидываем в предыдущее значение\n",
    "        self.prev_state = self.state\n",
    "        self.state = ''.join(flatten_obs)\n",
    "        self.prev_action = self.action\n",
    "        self.action = self.get_epsilon_greedy_action(self.qfunction[self.state], self.epsilon, self.action_dim, mask)\n",
    "        return self.action\n",
    "\n",
    "    def clear_state(self):\n",
    "        self.prev_state = ''\n",
    "        self.prev_action = -1\n",
    "        self.prev_reward = -1\n",
    "        self.state = ''\n",
    "        self.action = -1\n",
    "        self.reward = -1\n",
    "\n",
    "    def save_model_pickle(self, path='/home/artem/atari_games/sarsa.json'):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.qfunction.qdict, f)\n",
    "\n",
    "    def load_model_pickle(self, path='/home/artem/atari_games/sarsa.json'):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.qfunction.qdict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e11d60-6f9f-4e19-b717-9a297543e691",
   "metadata": {},
   "source": [
    "#### Обучаем стратегию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c8120628-9b7a-4be0-ad3d-cd6b670e0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(player, obs):\n",
    "    if player == \"player_1\":\n",
    "        obs = obs[\"observation\"]\n",
    "        x = obs[:,:,0]\n",
    "        o = obs[:,:,1]\n",
    "    else:\n",
    "        obs = obs[\"observation\"]\n",
    "        x = obs[:,:,1]\n",
    "        o = obs[:,:,0]\n",
    "\n",
    "    print(\"-------\")\n",
    "    for i in range(3):\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(3):\n",
    "            if x[i][j] == 1:\n",
    "                print(\"x|\", end=\"\")\n",
    "            elif o[i][j] == 1:\n",
    "                print(\"o|\", end=\"\")\n",
    "            else:\n",
    "                print(\" |\", end=\"\")\n",
    "        print(\"\\n--------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70fcdf3a-2515-45e2-86ae-450625064461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_policy(env, Agents, episode_n):\n",
    "    for i in range(episode_n):\n",
    "        if i % 5_000 == 0:\n",
    "            print(f'##################\\niteration: {i}\\n')\n",
    "            \n",
    "        Agents['player_1'].clear_state()\n",
    "        Agents['player_2'].clear_state()\n",
    "        env.reset()\n",
    "        for agent_id in env.agent_iter():\n",
    "            agent = Agents[agent_id]\n",
    "            \n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "    \n",
    "            if i % 5_000 == 0:\n",
    "                print_board(agent_id, observation)\n",
    "        \n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                \n",
    "            else:\n",
    "                action = agent.get_action(observation)\n",
    "                \n",
    "            agent.fit(reward, termination or truncation)\n",
    "            \n",
    "            if i % 5_000 == 0:\n",
    "                print(f\"agent_id: {agent_id}, reward: {reward}, qfunction[prev_state]: {agent.qfunction[agent.prev_state]}\\n qfunction[state]: {agent.qfunction[agent.state]}, epsilon={ np.round(agent.epsilon, 3)}\")\n",
    "                \n",
    "            env.step(action)\n",
    "    return Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d168dfd8-c61e-4038-ae79-b680505f5f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "iteration: 0\n",
      "\n",
      "-------\n",
      "| | | |\n",
      "--------\n",
      "| | | |\n",
      "--------\n",
      "| | | |\n",
      "--------\n",
      "agent_id: player_1, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| | | |\n",
      "--------\n",
      "| | | |\n",
      "--------\n",
      "|x| | |\n",
      "--------\n",
      "agent_id: player_2, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| | | |\n",
      "--------\n",
      "|o| | |\n",
      "--------\n",
      "|x| | |\n",
      "--------\n",
      "agent_id: player_1, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| | |x|\n",
      "--------\n",
      "|o| | |\n",
      "--------\n",
      "|x| | |\n",
      "--------\n",
      "agent_id: player_2, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| | |x|\n",
      "--------\n",
      "|o| | |\n",
      "--------\n",
      "|x| |o|\n",
      "--------\n",
      "agent_id: player_1, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| | |x|\n",
      "--------\n",
      "|o| | |\n",
      "--------\n",
      "|x|x|o|\n",
      "--------\n",
      "agent_id: player_2, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| | |x|\n",
      "--------\n",
      "|o|o| |\n",
      "--------\n",
      "|x|x|o|\n",
      "--------\n",
      "agent_id: player_1, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| |x|x|\n",
      "--------\n",
      "|o|o| |\n",
      "--------\n",
      "|x|x|o|\n",
      "--------\n",
      "agent_id: player_2, reward: 0, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.], epsilon=1.0\n",
      "-------\n",
      "| |x|x|\n",
      "--------\n",
      "|o|o|o|\n",
      "--------\n",
      "|x|x|o|\n",
      "--------\n",
      "agent_id: player_1, reward: -1, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [ 0.   -0.95  0.    0.    0.    0.    0.    0.    0.  ], epsilon=1.0\n",
      "-------\n",
      "| |x|x|\n",
      "--------\n",
      "|o|o|o|\n",
      "--------\n",
      "|x|x|o|\n",
      "--------\n",
      "agent_id: player_2, reward: 1, qfunction[prev_state]: [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " qfunction[state]: [0.   0.   0.   0.   0.   0.95 0.   0.   0.  ], epsilon=1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m agent\u001b[38;5;241m.\u001b[39mfit(reward, termination \u001b[38;5;129;01mor\u001b[39;00m truncation)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5_000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[89], line 50\u001b[0m, in \u001b[0;36mSARSAAgent.get_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(flatten_obs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_epsilon_greedy_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqfunction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction\n",
      "Cell \u001b[0;32mIn[89], line 26\u001b[0m, in \u001b[0;36mSARSAAgent.get_epsilon_greedy_action\u001b[0;34m(self, q_values, epsilon, action_n, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum(masked_policy)\n\u001b[1;32m     25\u001b[0m masked_policy \u001b[38;5;241m=\u001b[39m masked_policy \u001b[38;5;241m+\u001b[39m p \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p) \u001b[38;5;241m*\u001b[39m masked_policy\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m(action_n), p\u001b[38;5;241m=\u001b[39mmasked_policy)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = tictactoe_v3.env()\n",
    "\n",
    "episode_n = 60_000\n",
    "\n",
    "agent1 = SARSAAgent(alpha=0.95, episode_n=episode_n)\n",
    "agent2 = SARSAAgent(alpha=0.95, episode_n=episode_n)\n",
    "\n",
    "Agents = {'player_1': agent1, 'player_2': agent2}\n",
    "\n",
    "for i in range(episode_n):\n",
    "    if i % 5_000 == 0:\n",
    "        print(f'##################\\niteration: {i}\\n')\n",
    "    if i == episode_n // 2:\n",
    "        # смена игроков. Теперь начинает агент 2\n",
    "        Agents['player_1'] = agent2\n",
    "        Agents['player_2'] = agent1\n",
    "        agent1.epsilon = 1.0\n",
    "        agent2.epsilon = 1.0\n",
    "        agent1.episode = 0\n",
    "        agent2.episode = 0\n",
    "        agent1.episode_n = episode_n // 2\n",
    "        agent2.episode_n = episode_n // 2\n",
    "        \n",
    "    Agents['player_1'].clear_state()\n",
    "    Agents['player_2'].clear_state()\n",
    "    env.reset()\n",
    "    for agent_id in env.agent_iter():\n",
    "        agent = Agents[agent_id]\n",
    "        \n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if i % 5_000 == 0:\n",
    "            print_board(agent_id, observation)\n",
    "    \n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "            \n",
    "        else:\n",
    "            action = agent.get_action(observation)\n",
    "            \n",
    "        agent.fit(reward, termination or truncation)\n",
    "        \n",
    "        if i % 5_000 == 0:\n",
    "            print(f\"agent_id: {agent_id}, reward: {reward}, qfunction[prev_state]: {agent.qfunction[agent.prev_state]}\\n qfunction[state]: {agent.qfunction[agent.state]}, epsilon={ np.round(agent.epsilon, 3)}\")\n",
    "            \n",
    "        env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadfba4-e828-4129-aba7-908d047e95e9",
   "metadata": {},
   "source": [
    "#### Сохраняем модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb6d06df-29b9-4be1-95ba-663e69804d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.save_model_pickle('good_strategy1')\n",
    "agent2.save_model_pickle('good_strategy2')\n",
    "# agent1.save_model_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42248100-2c68-4533-89d7-2643c268544f",
   "metadata": {},
   "source": [
    "#### Проверяем в игре против рандомной стратегии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc4ea970-4d4d-4460-b3b4-8e7a49d4c473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player1 wins 0 lose 0 games from 1000 games\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env()\n",
    "env.reset(seed=45)\n",
    "\n",
    "agent1 = SARSAAgent()\n",
    "agent1.load_model_pickle(\"good_strategy2\")\n",
    "agent1.epsilon = 0.00001\n",
    "\n",
    "#agent2 = RandomAgent()\n",
    "\n",
    "agent2 = SARSAAgent()\n",
    "agent2.load_model_pickle(\"good_strategy1\")\n",
    "agent2.epsilon = 0.00001\n",
    "\n",
    "Agents = {'player_1': agent1, 'player_2': agent2}\n",
    "games_n = 1000\n",
    "wins = 0\n",
    "losses = 0\n",
    "for i in range(games_n):\n",
    "    j = 0\n",
    "    env.reset()\n",
    "    for agent_id in env.agent_iter():\n",
    "        agent = Agents[agent_id]\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        # print(reward)\n",
    "\n",
    "        if reward == 1 and j % 2 == 0:\n",
    "            wins += 1\n",
    "        elif reward == -1 and j % 2 == 0:\n",
    "            losses += 1\n",
    "        j += 1\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            action = agent.get_action(observation)\n",
    "    \n",
    "        env.step(action)\n",
    "        \n",
    "print(f'player1 wins {wins} lose {losses} games from {games_n} games')       \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b90b26b5-036b-40da-a1ea-45fe15727f8a",
   "metadata": {},
   "source": [
    "player1 wins 726 lose 90 games from 1000 games - последний полученный результат. Видно, что стратегия обучилась играть и выигрывает с большим заделом у рандомной стратегии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aac938-310d-4aa4-958f-c983fc58e46f",
   "metadata": {},
   "source": [
    "## Задача решена"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
