{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfac5314-9b4b-40f5-9f50-b3da46dc39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import play \n",
    "from gym import wrappers\n",
    "from gym.wrappers import GrayScaleObservation, RecordEpisodeStatistics, TimeLimit, ResizeObservation, FrameStack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hashlib import md5\n",
    "import os\n",
    "\n",
    "def apply_wrappers(env):\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    env = TimeLimit(env, 500)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2204a87d-7de5-40ac-ab05-361a43535430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3161/2570963249.py:139: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  obs = torch.tensor(obs, dtype=torch.float).squeeze().unsqueeze(dim=0)\n",
      "/tmp/ipykernel_3161/2570963249.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs, dtype=torch.float)\n",
      "/home/artem/atari_games/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(frame_size, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, episode_n\u001b[38;5;241m=\u001b[39mepisode_n)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#agent.load_model('/home/artem/atari_games/models/DQN_c11323c6ff7080195e86ee7b2e01d04b.pth')\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# state, info = env.reset()\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mDQN_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m agent\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/artem/atari_games/models/atari_model_1.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#env.close()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mDQN_learning\u001b[0;34m(agent, episode_n, t_max, visualize)\u001b[0m\n\u001b[1;32m     16\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     17\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 19\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m agent\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "Cell \u001b[0;32mIn[3], line 155\u001b[0m, in \u001b[0;36mDQNAgent.add_sample\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_all()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdrb\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    151\u001b[0m     TensorDict({\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(action),\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(reward),\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(done)\n\u001b[1;32m    157\u001b[0m             }, batch_size\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    158\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def DQN_learning(agent, episode_n = 100, t_max = 1000, visualize=False):\n",
    "    # agent.epsilon_decrease = 1.0 / (0.75*episode_n)\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "        total_reward = 0\n",
    "        if episode == 50 or episode == 200:\n",
    "            agent.save_model()\n",
    "\n",
    "        # print(agent.optimizer)\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        print(len(agent.tdrb))\n",
    "        for t in range(t_max):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.add_sample(state, action, reward, next_state, done or truncated)\n",
    "            agent.fit()\n",
    "\n",
    "            if visualize:\n",
    "                env.render()\n",
    "    \n",
    "            state = next_state\n",
    "            if done or truncated:\n",
    "                # print(info)\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(total_reward)\n",
    "        \n",
    "        print(f'episode: {episode}, total_reward: {info[\"episode\"][\"r\"]},\\\n",
    "        episode length: {info[\"episode\"][\"l\"]},\\\n",
    "        time: {info[\"episode\"][\"t\"]}')\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\",  obs_type='rgb', frameskip=4)\n",
    "env = apply_wrappers(env)\n",
    "\n",
    "# env.metadata['render_fps'] = 30\n",
    "episode_n = 500\n",
    "\n",
    "frame_size = []\n",
    "frame_size.extend(list(env.observation_space.shape))\n",
    "\n",
    "agent = DQNAgent(frame_size, env.action_space.n, episode_n=episode_n)\n",
    "#agent.load_model('/home/artem/atari_games/models/DQN_c11323c6ff7080195e86ee7b2e01d04b.pth')\n",
    "# state, info = env.reset()\n",
    "DQN_learning(agent, episode_n, visualize=False)\n",
    "agent.save_model('/home/artem/atari_games/models/atari_model_1.pth')\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68533e7b-d8b6-4961-8022-72783dcd0e6d",
   "metadata": {},
   "source": [
    "#### Тестируем PrioritizedBuffer на LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6c8781-1c32-43c2-b916-9026980bb98c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_rewards\n\u001b[1;32m     39\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mapply_wrappers\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m, in \u001b[0;36mapply_wrappers\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_wrappers\u001b[39m(env):\n\u001b[0;32m---> 12\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mGrayScaleObservation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     env \u001b[38;5;241m=\u001b[39m ResizeObservation(env, (\u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m))\n\u001b[1;32m     14\u001b[0m     env \u001b[38;5;241m=\u001b[39m FrameStack(env, num_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/gym/wrappers/gray_scale_observation.py:34\u001b[0m, in \u001b[0;36mGrayScaleObservation.__init__\u001b[0;34m(self, env, keep_dim)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_dim \u001b[38;5;241m=\u001b[39m keep_dim\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, Box)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_dim:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def DQN_learning(agent, episode_n = 100, t_max = 1000):\n",
    "    # agent.epsilon_decrease = 1.0 / (0.75*episode_n)\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "        total_reward = 0\n",
    "\n",
    "        # print(agent.optimizer)\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        print(len(agent.tdrb))\n",
    "        for t in range(t_max):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "\n",
    "            agent.add_sample(state, action, reward, next_state, done or truncated)\n",
    "            agent.fit()\n",
    "    \n",
    "            state = next_state\n",
    "    \n",
    "            if done or truncated:\n",
    "                # print(info)\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(total_reward)\n",
    "        \n",
    "        print(f'episode: {episode}, total_reward: {info[\"episode\"][\"r\"]},\\\n",
    "        episode length: {info[\"episode\"][\"l\"]},\\\n",
    "        time: {info[\"episode\"][\"t\"]}')\n",
    "    \n",
    "    return total_rewards\n",
    "    \n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env = apply_wrappers(env)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "episode_n = 1000\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, episode_n=episode_n)\n",
    "\n",
    "total_rewards = DQN_learning(agent, episode_n=episode_n)\n",
    "agent.save_model()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9177b-6b06-4529-a4a2-891c711d2094",
   "metadata": {},
   "source": [
    "#### Visual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed53f0d4-0dd8-4876-8e0f-c7ff4e8d4afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/atari_games/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode': {'r': 281.86694, 'l': 235, 't': 9.974647}}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env = apply_wrappers(env)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "agent.load_model('/home/artem/atari_games/models/DQN_da6ad5af96c552ca0ecc3e1d5325959d.pth')\n",
    "agent.epsilon = 0.001\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    if done or truncated:\n",
    "        print(info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5504b1a4-b021-4b46-80dd-af7cf9512514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tensordict import TensorDict\n",
    "\n",
    "import torchrl\n",
    "from torchrl.data.replay_buffers import PrioritizedReplayBuffer, ReplayBuffer, PrioritizedSampler\n",
    "from torchrl.data import TensorDictPrioritizedReplayBuffer, LazyMemmapStorage, TensorDictReplayBuffer\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    def __init__(self, input_shape, action_n):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.action_n = action_n\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_output_size = self._get_conv_output_size()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            self.conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_n)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_size(self):\n",
    "        out = self.conv_layers(torch.zeros(1, *self.input_shape))\n",
    "        return np.prod(out.shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class QFunction_baseline(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, action_dim)                \n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        actions = self.model(states)\n",
    "        return actions\n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, input_shape, action_n, lr=1e-3, gamma=0.99, batch_size=128, period=25, episode_n=1000):\n",
    "        super().__init__()\n",
    "        self.q_function = QFunction(input_shape, action_n)\n",
    "        self.target_q_function = QFunction(input_shape, action_n)\n",
    "\n",
    "        self.m = 5.0\n",
    "\n",
    "        self.epsilon_min = 5e-3\n",
    "        self.epsilon_decay = self.m * 1.0 / (episode_n)\n",
    "        self.epsilon = 1.0 - 1.0 / action_n\n",
    "\n",
    "        self.episode_n = episode_n\n",
    "        self.current_episode = 0\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.action_n = action_n\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.period = period\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[int(0.4 * self.episode_n), int(0.6 * self.episode_n), int(0.8*self.episode_n)], gamma=0.5)\n",
    "        self.loss_f = nn.MSELoss()\n",
    "\n",
    "        self.alpha = 0.8\n",
    "        self.alpha_decay = self.m * 1.0 / episode_n\n",
    "        self.beta = 0.3\n",
    "        self.beta_decay = self.m * 1.0 / episode_n\n",
    "\n",
    "        self.capacity = 400_000\n",
    "        storage = LazyMemmapStorage(self.capacity, scratch_dir='/home/artem/atari_games/tmp/')\n",
    "        self.sampler = PrioritizedSampler(self.capacity, alpha=self.alpha, beta=self.beta)\n",
    "        self.tdrb = TensorDictReplayBuffer(storage=storage, sampler=self.sampler, priority_key='td_error')\n",
    "\n",
    "    def save_model(self, path=f'/home/artem/atari_games/models/DQN_{md5(str(time.time()).encode()).hexdigest()}.pth'):\n",
    "        state = {\n",
    "            'model_dict': self.q_function.state_dict(),\n",
    "            'optimizer_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "\n",
    "    def load_model(self,path=f'/home/artem/atari_games/models/DQN_{md5(str(time.time()).encode()).hexdigest()}'):\n",
    "        if os.path.exists(path):\n",
    "            state = torch.load(path)\n",
    "            self.optimizer.load_state_dict(state['optimizer_dict'])\n",
    "            self.q_function.load_state_dict(state['model_dict'])\n",
    "            self.target_q_function.load_state_dict(state['model_dict'])\n",
    "            self.epsilon = state['epsilon']\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max((self.epsilon - self.epsilon_decay), self.epsilon_min)\n",
    "\n",
    "    def decay_alpha(self):\n",
    "        self.aplha = max(self.alpha - self.alpha_decay, 0.7)\n",
    "        self.sampler._alpha = self.alpha\n",
    "\n",
    "    def decay_beta(self):\n",
    "        self.beta = min(self.beta + self.beta_decay, 0.5)\n",
    "        self.sampler._beta = self.beta\n",
    "\n",
    "    def decay_all(self):\n",
    "        self.decay_epsilon()\n",
    "        self.decay_alpha()\n",
    "        self.decay_beta()\n",
    "\n",
    "    def e_greedy_action(self, q_values):\n",
    "        probs = np.ones(self.action_n) * self.epsilon / self.action_n\n",
    "        probs[np.argmax(q_values.numpy())] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_n), p=probs)\n",
    "        return action\n",
    "        \n",
    "    def get_action(self, obs: np.ndarray) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, dtype=torch.float).squeeze().unsqueeze(dim=0)\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "            q_values = self.q_function(obs).squeeze()\n",
    "            action = self.e_greedy_action(q_values)\n",
    "            return action\n",
    "\n",
    "    def add_sample(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            self.current_episode += 1\n",
    "            self.scheduler.step()\n",
    "            self.decay_all()\n",
    "        self.tdrb.add(\n",
    "            TensorDict({\n",
    "                'state': torch.tensor(state, dtype=torch.float32).squeeze(),\n",
    "                'action': torch.tensor(action),\n",
    "                'reward': torch.tensor(reward),\n",
    "                'next_state': torch.tensor(next_state, dtype=torch.float32).squeeze(),\n",
    "                'done': torch.tensor(done)\n",
    "                    }, batch_size=[])\n",
    "        )\n",
    "        # self.tdrb.add(\n",
    "        #     TensorDict({\n",
    "        #         'state': torch.tensor(state, dtype=torch.float32),\n",
    "        #         'action': torch.tensor(action),\n",
    "        #         'reward': torch.tensor(reward),\n",
    "        #         'next_state': torch.tensor(next_state, dtype=torch.float32),\n",
    "        #         'done': torch.tensor(done)\n",
    "        #             }, batch_size=[])\n",
    "        # )\n",
    "    \n",
    "    def fit(self):\n",
    "        if self.batch_size < len(self.tdrb):\n",
    "            # print('fit')\n",
    "            if self.counter == self.period:\n",
    "                self.counter = 0\n",
    "                for parameter_freeze, parameter in zip(self.target_q_function.model.parameters(), self.q_function.model.parameters()):\n",
    "                    with torch.no_grad():\n",
    "                        parameter_freeze.data.copy_(parameter.data)\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "            sample = self.tdrb.sample(self.batch_size)\n",
    "\n",
    "            \n",
    "            # target = sample['reward'] + self.gamma * (1 - torch.tensor(sample['done'], dtype=torch.uint8)) * \\\n",
    "            # (self.target_q_function(sample['next_state'])[torch.arange(self.batch_size), torch.argmax(self.q_function(sample['next_state']), dim=1)])\n",
    "            # print(sample['next_state'].shape)\n",
    "            # print(self.target_q_function(sample['next_state']).shape)\n",
    "            # print(torch.argmax(self.q_function(sample['next_state']), dim=1).shape)\n",
    "            target = sample['reward'] + self.gamma * (1 - torch.tensor(sample['done'], dtype=torch.uint8)) * \\\n",
    "            (self.target_q_function(sample['next_state'])[torch.arange(self.batch_size), torch.argmax(self.q_function(sample['next_state']), dim=1)])\n",
    "            \n",
    "            q_values = self.q_function(sample['state'])[torch.arange(self.batch_size), sample['action']]\n",
    "\n",
    "            td_error = q_values - target.detach()\n",
    "            sample['td_error'] = td_error.abs().detach()\n",
    "            \n",
    "            loss = torch.mean((td_error * sample['_weight'])**2)\n",
    "            # loss = torch.mean((td_error)**2)\n",
    "            self.tdrb.update_tensordict_priority(sample)\n",
    "            \n",
    "            loss.backward()\n",
    "            # print('after update')\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            # self.decay_all()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb89ab7-747a-4384-8185-b46cb6788f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa49af65-fd92-4c5f-855d-ef12d788217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20390/1352533337.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  1 - torch.tensor(torch.tensor([True, False]), dtype=torch.uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - torch.tensor(torch.tensor([True, False]), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5db0469-f39b-40be-bcb6-4103c29caa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([210, 160])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20689/796179002.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs, dtype=torch.float).unsqueeze(dim=0).unsqueeze(dim=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent =  AtariAgent((1, 210, 160), 2)\n",
    "\n",
    "v = torch.ones((210, 160), dtype=torch.uint8)\n",
    "\n",
    "print(v.shape)\n",
    "agent.get_action(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac20d082-f3d8-47fc-bb44-37cc6f1c2add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33600])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones(1,1, 210, 160)\n",
    "new_t = torch.flatten(t)\n",
    "new_t.shape\n",
    "\n",
    "f = nn.Flatten()\n",
    "f(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f979ca-8634-4f6c-96d5-8b2d8dd755b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "network = nn.Sequential(\n",
    "            conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(5632, 512)\n",
    ")\n",
    "\n",
    "network(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31fd4406-02af-4518-9342-c18e626ed645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([0, 1] ,p=[0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edef19fa-5c45-4d8e-bb55-4050a1d906fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 2, 3]]).numpy()\n",
    "np.argmax(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "739b9991-128a-4e31-b550-6e203201bcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3359, 0.0023, 0.9459, 0.0583],\n",
       "        [0.3100, 0.2059, 0.6849, 0.7648],\n",
       "        [0.5246, 0.1823, 0.9524, 0.9177]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec113f98-2067-4dba-8b95-a817babf89bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 2, 2, 4, 1, 2, 4, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rp = ReplayBuffer()\n",
    "rp.add(1)\n",
    "rp.add(2)\n",
    "rp.extend([3, 4])\n",
    "batch = rp.sample(batch_size=10)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc09f3d3-dacf-43d4-ab02-adcf494d8161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "TensorDict(\n",
      "    fields={\n",
      "        _data: TensorDict(\n",
      "            fields={\n",
      "                a: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                b: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 4, 4, 3, 2, 4, 4, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdrp = TensorDictReplayBuffer()\n",
    "\n",
    "dt0 = TensorDict({'a': torch.tensor([1]), 'b': torch.tensor([2])}, batch_size=[])\n",
    "dt1 = TensorDict({'a': torch.rand(4, 1), 'b': torch.rand(4, 2)}, batch_size=[4])\n",
    "\n",
    "tdrp.add(dt0)\n",
    "tdrp.extend(dt1)\n",
    "print(len(tdrp))\n",
    "print(tdrp[0])\n",
    "\n",
    "sample = tdrp.sample(batch_size=10)\n",
    "sample['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7ec91fe-6c1d-4eb0-baea-175174f05868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "4\n",
      "TensorDict(\n",
      "    fields={\n",
      "        _data: TensorDict(\n",
      "            fields={\n",
      "                a: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                b: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "tensor([3, 0, 1])\n",
      "tensor([0, 3, 2])\n",
      "tensor([1.0000, 0.7361, 0.7588])\n"
     ]
    }
   ],
   "source": [
    "tdrp = TensorDictReplayBuffer(priority_key='td_error', sampler=PrioritizedSampler(max_capacity=4, alpha=100.0, beta=1.1))\n",
    "tdrp.set_sampler(sampler=PrioritizedSampler(max_capacity=100, alpha=0.7, beta=0.5))\n",
    "#dt0 = TensorDict({'a': torch.tensor([1]), 'b': torch.tensor([2])}, batch_size=[])\n",
    "dt1 = TensorDict({'a': torch.rand(4, 1), 'b': torch.rand(4, 2)}, batch_size=[4])\n",
    "\n",
    "#tdrp.add(dt0)\n",
    "indeces = tdrp.extend(dt1)\n",
    "print(indeces)\n",
    "print(len(tdrp))\n",
    "print(tdrp[0])\n",
    "\n",
    "sample = tdrp.sample(batch_size=3)\n",
    "print(sample['index'])\n",
    "sample\n",
    "\n",
    "tdrp.update_priority(indeces, priority=np.array([0.5, 0.8, 1.1, 1.2]))\n",
    "sample = tdrp.sample(batch_size=3)\n",
    "print(sample['index'])\n",
    "#print(sample)\n",
    "# tdrp.set_sampler(sampler=PrioritizedSampler(max_capacity=100, alpha=0.01, beta=1.1))\n",
    "#sample = tdrp.sample(batch_size=10)\n",
    "print(sample['_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e080798-a1a4-4bc6-bfcc-7d6d771f634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 2])\n",
      "tensor([[0.1765],\n",
      "        [0.9584],\n",
      "        [0.7441]])\n"
     ]
    }
   ],
   "source": [
    "sample = tdrp.sample(batch_size=3)\n",
    "print(sample['index'])\n",
    "#print(sample)\n",
    "# tdrp.set_sampler(sampler=PrioritizedSampler(max_capacity=100, alpha=0.01, beta=1.1))\n",
    "#sample = tdrp.sample(batch_size=10)\n",
    "print(sample[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7f0dbc82-8ce7-457b-b908-6611a19aa196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt2 = TensorDict({'a': torch.rand(1, 1), 'b': torch.rand(1, 2)}, batch_size=[1])\n",
    "tdrp.add(dt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16c6aae1-a4a7-40d8-ad33-57ba30c1ca4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stacking tensordicts requires them to have congruent batch sizes, got td1.batch_size=torch.Size([]) and td2.batch_size=torch.Size([1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtdrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(sample)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# tdrp.set_sampler(sampler=PrioritizedSampler(max_capacity=100, alpha=0.01, beta=1.1))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#sample = tdrp.sample(batch_size=10)\u001b[39;00m\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/torchrl/data/replay_buffers/replay_buffers.py:964\u001b[0m, in \u001b[0;36mTensorDictReplayBuffer.sample\u001b[0;34m(self, batch_size, return_info, include_info)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_info is going to be deprecated soon.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default behaviour has changed to `include_info=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto avoid bugs linked to wrongly preassigned values in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput tensordict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m data, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m is_tc \u001b[38;5;241m=\u001b[39m is_tensor_collection(data)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tc \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensorclass(data) \u001b[38;5;129;01mand\u001b[39;00m include_info \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/torchrl/data/replay_buffers/replay_buffers.py:520\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size, return_info)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size not specified. You can specify the batch_size when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstructing the replay buffer, or pass it to the sample method. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to the ReplayBuffer documentation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor a proper usage of the batch-size arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch:\n\u001b[0;32m--> 520\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_futures_lock:\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/torchrl/data/replay_buffers/utils.py:49\u001b[0m, in \u001b[0;36mpin_memory_output.<locals>.decorated_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorated_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 49\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m     51\u001b[0m         _tuple_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/torchrl/data/replay_buffers/replay_buffers.py:461\u001b[0m, in \u001b[0;36mReplayBuffer._sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    459\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage\u001b[38;5;241m.\u001b[39mget(index)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, INT_CLASSES):\n\u001b[0;32m--> 461\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform):\n\u001b[1;32m    463\u001b[0m     is_td \u001b[38;5;241m=\u001b[39m is_tensor_collection(data)\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/torchrl/data/replay_buffers/storages.py:1124\u001b[0m, in \u001b[0;36m_collate_list_tensordict\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1122\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tensor_collection(out):\n\u001b[0;32m-> 1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_reset_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/torchrl/data/replay_buffers/storages.py:1115\u001b[0m, in \u001b[0;36m_reset_batch_size\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         data\u001b[38;5;241m.\u001b[39mlock_()\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m-> 1115\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/tensordict/base.py:2256\u001b[0m, in \u001b[0;36mTensorDictBase.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[1;32m   2255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(_GENERIC_NESTED_ERR\u001b[38;5;241m.\u001b[39mformat(key))\n\u001b[0;32m-> 2256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/tensordict/_lazy.py:850\u001b[0m, in \u001b[0;36mLazyStackedTensorDict._get_tuple\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default):\n\u001b[0;32m--> 850\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_get(key[\u001b[38;5;241m0\u001b[39m], default)\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/tensordict/utils.py:1015\u001b[0m, in \u001b[0;36mcache.<locals>.newfun\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fun)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnewfun\u001b[39m(_self: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorDictBase\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _self\u001b[38;5;241m.\u001b[39mis_locked:\n\u001b[0;32m-> 1015\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m     cache \u001b[38;5;241m=\u001b[39m _self\u001b[38;5;241m.\u001b[39m_cache\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/tensordict/_lazy.py:847\u001b[0m, in \u001b[0;36mLazyStackedTensorDict._get_str\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound more than one unique shape in the tensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacked (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshapes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This is likely due to a modification \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m     )\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 847\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/tensordict/_lazy.py:804\u001b[0m, in \u001b[0;36mLazyStackedTensorDict._get_str\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 804\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_tensor_collection(out\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, LazyStackedTensorDict):\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# then it's a LazyStackedTD\u001b[39;00m\n",
      "File \u001b[0;32m~/atari_games/lib/python3.8/site-packages/tensordict/_lazy.py:905\u001b[0m, in \u001b[0;36mLazyStackedTensorDict.lazy_stack\u001b[0;34m(cls, items, dim, device, out)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m td \u001b[38;5;129;01min\u001b[39;00m items[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m td\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m!=\u001b[39m items[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 905\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacking tensordicts requires them to have congruent batch sizes, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    907\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot td1.batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtd\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and td2.batch_size=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    908\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitems[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m         )\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# We need to handle tensordicts with exclusive keys and tensordicts with\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# mismatching shapes.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# The first case is handled within _check_keys which fails if keys\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;66;03m# don't match exactly.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;66;03m# The second requires a check over the tensor shapes.\u001b[39;00m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyStackedTensorDict(\u001b[38;5;241m*\u001b[39mitems, stack_dim\u001b[38;5;241m=\u001b[39mdim)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stacking tensordicts requires them to have congruent batch sizes, got td1.batch_size=torch.Size([]) and td2.batch_size=torch.Size([1])"
     ]
    }
   ],
   "source": [
    "sample = tdrp.sample(batch_size=5)\n",
    "print(sample['index'])\n",
    "#print(sample)\n",
    "# tdrp.set_sampler(sampler=PrioritizedSampler(max_capacity=100, alpha=0.01, beta=1.1))\n",
    "#sample = tdrp.sample(batch_size=10)\n",
    "print(sample[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ac54ccd3-7cfd-46e6-ad4b-f85047af67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (sample['a'] - torch.tensor([0.002, 1.0, 10.0]).reshape(3,1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "65ea7997-3632-4f43-aacc-5b762571bff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.0494e-01],\n",
       "        [9.6114e-03],\n",
       "        [9.1567e+01]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a6e22430-d79b-4cb4-a19c-ad6afc2fc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['td_error'] = loss.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ca264e8b-4664-41ce-b654-2c808d687541",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdrp.update_tensordict_priority(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3a67547f-c86d-4d04-988f-20dcd325d1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "sample = tdrp.sample(batch_size=3)\n",
    "print(sample['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e0ff831-e36c-47f8-84b7-3511fa959332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000e-03],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+01]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.002, 1.0, 10.0]).reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "428b4e7c-63da-4606-93cb-00f7452474f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f206ad3-f0ed-4562-bf4e-894bfafebedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "4\n",
      "TensorDict(\n",
      "    fields={\n",
      "        _data: TensorDict(\n",
      "            fields={\n",
      "                a: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                b: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "tensor([2, 0, 1, 1, 3, 1, 1, 2, 0, 0])\n",
      "tensor([2, 0, 1, 0, 0, 1, 2, 2, 1, 0])\n",
      "TensorDict(\n",
      "    fields={\n",
      "        _weight: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        a: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        b: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        index: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "tdrp = TensorDictPrioritizedReplayBuffer(priority_key='td_error', alpha=0.8, beta=1.1)\n",
    "\n",
    "#dt0 = TensorDict({'a': torch.tensor([1]), 'b': torch.tensor([2])}, batch_size=[])\n",
    "dt1 = TensorDict({'a': torch.rand(4, 1), 'b': torch.rand(4, 2)}, batch_size=[4])\n",
    "\n",
    "#tdrp.add(dt0)\n",
    "indeces = tdrp.extend(dt1)\n",
    "print(indeces)\n",
    "print(len(tdrp))\n",
    "print(tdrp[0])\n",
    "\n",
    "sample = tdrp.sample(batch_size=10)\n",
    "print(sample['index'])\n",
    "\n",
    "tdrp.update_priority(indeces, priority=np.arange(3, -1, -1))\n",
    "sample = tdrp.sample(batch_size=10)\n",
    "print(sample['index'])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbcbc2c-65dd-472d-b408-a2175305fa26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
