{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e927c7b1-6575-4a8c-b5e8-01d88249ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import play \n",
    "from gym import wrappers\n",
    "from gym.wrappers import GrayScaleObservation, RecordEpisodeStatistics, TimeLimit, ResizeObservation, FrameStack\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hashlib import md5\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tensordict import TensorDict\n",
    "\n",
    "import torchrl\n",
    "from torchrl.data.replay_buffers import PrioritizedReplayBuffer, ReplayBuffer, PrioritizedSampler\n",
    "from torchrl.data import TensorDictPrioritizedReplayBuffer, LazyMemmapStorage, TensorDictReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe5c9fd-848a-42d6-b33e-c29e8d96e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskVelocityWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Gym environment observation wrapper used to mask velocity terms in\n",
    "    observations. The intention is the make the MDP partially observatiable.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(MaskVelocityWrapper, self).__init__(env)\n",
    "        if ENV == \"CartPole-v1\":\n",
    "            self.mask = np.array([1., 0., 1., 0.])\n",
    "        elif ENV == \"Pendulum-v0\":\n",
    "            self.mask = np.array([1., 1., 0.])\n",
    "        elif ENV == \"LunarLander-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        elif ENV == \"LunarLanderContinuous-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return  observation * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30561323-8533-41c0-a7f1-a0f08707870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RQfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(state_dim, hidden_size=hidden_size, num_layers=1)\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.prev_hidden_state = None\n",
    "        self.hidden_state = None\n",
    "        self.current_batch_size = 0\n",
    "\n",
    "    def initialize(self, batch_size):\n",
    "        self.current_batch_size = batch_size\n",
    "        if batch_size == 0:\n",
    "            self.hidden_state = (torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size))\n",
    "        else:\n",
    "            self.hidden_state = (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))\n",
    "        self.prev_hidden_state = None\n",
    "\n",
    "    def forward(self, x, batch_size=None):\n",
    "        if len(x.shape) == 3:\n",
    "            batch_size = x.shape[0]\n",
    "        if len(x.shape) == 2:\n",
    "            batch_size = 0\n",
    "        if self.hidden_state is None or batch_size != self.current_batch_size:\n",
    "            self.initialize(batch_size)\n",
    "\n",
    "        self.prev_hidden_state = self.hidden_state\n",
    "        output, self.hidden_state = self.rnn(x, self.hidden_state)\n",
    "        q_values = self.q(output)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# class RecurrentActor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, hidden_size=64, batch_size=64):\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64d81145-3912-42d0-ab8f-1305f1e068b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQNAgent(nn.Module):\n",
    "    def __init__(self, input_shape, action_n, lr=1e-3, gamma=0.9, batch_size=5, period=10, N=15, M=0, episode_n=1000):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.q_function = RQfunction(input_shape, action_n)\n",
    "        self.target_q_function = RQfunction(input_shape, action_n)\n",
    "        self.update_weights()\n",
    "\n",
    "        self.epsilon_min = 5e-3\n",
    "        self.epsilon_decay = 1.0 / (episode_n)\n",
    "        self.epsilon = 0.7\n",
    "\n",
    "        self.episode_n = episode_n\n",
    "        self.current_episode = 0\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.action_n = action_n\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.period = period\n",
    "        self.counter = 1\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[int(0.8 * self.episode_n)], gamma=0.5)\n",
    "\n",
    "        # self.capacity = 100_000\n",
    "        # storage = LazyMemmapStorage(self.capacity, scratch_dir='/home/artem/atari_games/tmp/')\n",
    "        # self.sampler = PrioritizedSampler(self.capacity, alpha=self.alpha, beta=self.beta)\n",
    "        # self.tdrb = TensorDictReplayBuffer(storage=storage, sampler=self.sampler, priority_key='td_error')\n",
    "        self.states_count = 0\n",
    "        self.rb = []\n",
    "\n",
    "        self.states = None\n",
    "        self.hidden_states =  None\n",
    "        self.actions = None\n",
    "        self.rewards = None\n",
    "        self.dones = None\n",
    "        self.next_states = None\n",
    "        self.next_hidden_states = None\n",
    "\n",
    "    def save_model(self, path=f'/home/artem/atari_games/models/DRQN_{md5(str(time.time()).encode()).hexdigest()}.pth'):\n",
    "        state = {\n",
    "            'model_dict': self.q_function.state_dict(),\n",
    "            'optimizer_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "\n",
    "    def load_model(self,path=f'/home/artem/atari_games/models/DRQN_{md5(str(time.time()).encode()).hexdigest()}'):\n",
    "        if os.path.exists(path):\n",
    "            state = torch.load(path)\n",
    "            self.optimizer.load_state_dict(state['optimizer_dict'])\n",
    "            self.q_function.load_state_dict(state['model_dict'])\n",
    "            self.target_q_function.load_state_dict(state['model_dict'])\n",
    "            self.epsilon = state['epsilon']\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max((self.epsilon - self.epsilon_decay), self.epsilon_min)\n",
    "\n",
    "    def decay_all(self):\n",
    "        self.decay_epsilon()\n",
    "\n",
    "    def e_greedy_action(self, q_values):\n",
    "        probs = np.ones(self.action_n) * self.epsilon / self.action_n\n",
    "        probs[np.argmax(q_values.numpy())] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_n), p=probs)\n",
    "        return action\n",
    "        \n",
    "    def get_action(self, obs: np.ndarray) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, dtype=torch.float).unsqueeze(dim=0)\n",
    "            q_values = self.q_function(obs).squeeze()\n",
    "            if self.counter % 1500 == 0:\n",
    "                print(q_values)\n",
    "            action = self.e_greedy_action(q_values)\n",
    "            return action\n",
    "\n",
    "    def add_sample(self, state, action, reward, next_state, done):\n",
    "        # print(f\"stacked {torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=0)}\")\n",
    "        # print(f\"hidden_states {0 if self.hidden_states is None else self.hidden_states.shape}\")\n",
    "\n",
    "        stacked_hidden_state = torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=0)\n",
    "        stacked_next_hidden_state = torch.stack((self.q_function.hidden_state[0], self.q_function.hidden_state[1]), dim=0)\n",
    "\n",
    "        self.states = torch.tensor(state).unsqueeze(dim=0) if self.states is None else torch.cat((self.states, torch.tensor(state).unsqueeze(dim=0)), dim=0)\n",
    "        # self.hidden_states.append(torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=1))\n",
    "        self.hidden_states = stacked_hidden_state.unsqueeze(dim=0) if self.hidden_states is None else torch.cat((self.hidden_states, stacked_hidden_state.unsqueeze(dim=0)), dim=0)\n",
    "        self.actions = torch.tensor(action).unsqueeze(dim=0) if self.actions is None else torch.cat((self.actions, torch.tensor(action).unsqueeze(dim=0)), dim=0)\n",
    "        self.rewards = torch.tensor(reward).unsqueeze(dim=0) if self.rewards is None else torch.cat((self.rewards, torch.tensor(reward).unsqueeze(dim=0)), dim=0)\n",
    "        self.dones = torch.tensor(int(done)).unsqueeze(dim=0) if self.dones is None else torch.cat((self.dones, torch.tensor(int(done)).unsqueeze(dim=0)), dim=0)\n",
    "        self.next_states = torch.tensor(next_state).unsqueeze(dim=0) if self.next_states is None else torch.cat((self.next_states, torch.tensor(next_state).unsqueeze(dim=0)), dim=0)\n",
    "        self.next_hidden_states = stacked_next_hidden_state.unsqueeze(dim=0) if self.next_hidden_states is None else torch.cat((self.next_hidden_states, stacked_next_hidden_state.unsqueeze(dim=0)), dim=0)\n",
    "        \n",
    "        self.states_count += 1\n",
    "\n",
    "        if done or self.states_count % self.N == 0:\n",
    "            # print(self.states.shape)\n",
    "            self.rb.append(\n",
    "                {\n",
    "                    'state': torch.tensor(self.states, dtype=torch.float32),\n",
    "                    'hidden_state': torch.tensor(self.hidden_states, dtype=torch.float32),\n",
    "                    'action': self.actions,\n",
    "                    'reward': self.rewards,\n",
    "                    'next_state': torch.tensor(self.next_states, dtype=torch.float32),\n",
    "                    'next_hidden_state': torch.tensor(self.next_hidden_states, dtype=torch.float32),\n",
    "                    'done': torch.tensor(self.dones, dtype=torch.float32)\n",
    "                        }\n",
    "            )\n",
    "\n",
    "            self.states = None\n",
    "            self.hidden_states =  None\n",
    "            self.actions = None\n",
    "            self.rewards = None\n",
    "            self.dones = None\n",
    "            self.next_states = None\n",
    "            self.next_hidden_states = None\n",
    "\n",
    "        if done:\n",
    "            self.current_episode += 1\n",
    "            self.decay_all()\n",
    "            self.q_function.prev_hidden_state = None\n",
    "            self.q_function.hidden_state = None\n",
    "\n",
    "    def update_weights(self):\n",
    "        for parameter_freeze, parameter in zip(self.target_q_function.rnn.parameters(), self.q_function.rnn.parameters()):\n",
    "            with torch.no_grad():\n",
    "                parameter_freeze.data.copy_(parameter.data)\n",
    "        for parameter_freeze, parameter in zip(self.target_q_function.q.parameters(), self.q_function.q.parameters()):\n",
    "            with torch.no_grad():\n",
    "                parameter_freeze.data.copy_(parameter.data)\n",
    "\n",
    "    def fit(self):\n",
    "        # print(len(self.rb))\n",
    "        if self.batch_size < len(self.rb):\n",
    "            # print('fit')\n",
    "            if self.counter % self.period == 0:\n",
    "                # print('weights change')\n",
    "                # не уверен, что LSTM можно так легко скопировать\n",
    "                self.update_weights()\n",
    "                \n",
    "            self.counter += 1\n",
    "\n",
    "            sample = random.sample(self.rb, self.batch_size)\n",
    "\n",
    "            flag = True\n",
    "            for rollout in sample:\n",
    "                # print(rollout['state'])\n",
    "                # вычисляем таргеты\n",
    "                self.q_function.hidden_state = (rollout['hidden_state'][0][0], rollout['hidden_state'][0][1])\n",
    "                self.target_q_function.hidden_state = (rollout['hidden_state'][0][0], rollout['hidden_state'][0][1])\n",
    "\n",
    "                # print(f\"q_func: {self.target_q_function(rollout['next_state'])}\")\n",
    "                targets = rollout['reward'].unsqueeze(dim=1) + (1 - rollout['done'].unsqueeze(dim=1)) * self.gamma * self.target_q_function(rollout['next_state'])\\\n",
    "                .gather(1, torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1))\n",
    "\n",
    "                \n",
    "                # print(f\"done: {(1 - rollout['done']).shape}\")\n",
    "                # print(f\"shape check: {torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1).shape}\")\n",
    "                # print(f\"target q: {self.target_q_function(rollout['next_state']).shape}\")\n",
    "                # print(f\"the whole: {self.target_q_function(rollout['next_state']).gather(1, torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1)).shape}\")\n",
    "                # print(f\"targets check: {targets.shape}\")\n",
    "                # вычисляем loss \n",
    "                q_values = self.q_function(rollout['state']).gather(1, rollout['action'].unsqueeze(dim=1))\n",
    "\n",
    "                # получаем последние  N - M состояний, так как их отображение q_values более правдоподобно из-за прогрева hidden_state\n",
    "                if targets.shape[0] <= self.M:\n",
    "                    burned_targets = targets\n",
    "                    burned_q_values = q_values\n",
    "                else:\n",
    "                    burned_targets = targets[self.M:]\n",
    "                    burned_q_values = q_values[self.M:]\n",
    "                \n",
    "                loss = torch.mean((burned_q_values - burned_targets.detach()) ** 2)\n",
    "                loss.backward()\n",
    "\n",
    "                # nn.utils.clip_grad_norm_(self.q_function.q.parameters(), max_norm=100.0)\n",
    "                # nn.utils.clip_grad_norm_(self.q_function.rnn.parameters(), max_norm=100.0)\n",
    "\n",
    "                if self.counter % 1000 == 0 and flag:\n",
    "                    # print(f\"target q: {self.target_q_function(rollout['next_state'])}\")\n",
    "                    print(f\"reward: {rollout['reward']}\")\n",
    "                    print(f\"targets: {targets.shape}\")\n",
    "                    print(f\"burned_targets: {burned_targets.shape}\")\n",
    "                    print(f\"q_values: {q_values}\")\n",
    "                    print(f\"loss: {loss}\")\n",
    "                    print(self.q_function.q[0].weight.grad)\n",
    "                    flag = False\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            self.scheduler.step()\n",
    "            # self.q_function.hidden_state = None\n",
    "            # self.target_q_function.hidden_state = None\n",
    "                \n",
    "            \n",
    "            # target = sample['reward'] + self.gamma * (1 - torch.tensor(sample['done'], dtype=torch.uint8)) * \\\n",
    "            # (self.target_q_function(sample['next_state'])[torch.arange(self.batch_size), torch.argmax(self.q_function(sample['next_state']), dim=1)])\n",
    "            # print(sample['next_state'].shape)\n",
    "            # print(self.target_q_function(sample['next_state']).shape)\n",
    "            # print(torch.argmax(self.q_function(sample['next_state']), dim=1).shape)\n",
    "            # print(sample['next_state'].shape)\n",
    "            # print(self.q_function(sample['next_state'].unsqueeze(dim=0) , self.batch_size).shape)\n",
    "            # print(torch.argmax(self.q_function(sample['next_state'].unsqueeze(dim=0), self.batch_size), dim=1).shape)\n",
    "            # target = sample['reward'] + self.gamma * (1 - torch.tensor(sample['done'], dtype=torch.uint8)) * \\\n",
    "            # (self.q_function(sample['next_state'].unsqueeze(dim=0) , self.batch_size).gather(1, torch.argmax(self.q_function(sample['next_state'].unsqueeze(dim=0), self.batch_size), dim=1).unsqueeze(dim=1)))\n",
    "            \n",
    "            # q_values = self.q_function(sample['state'].unsqueeze(dim=0), self.batch_size).gather(1, sample['action'].unsqueeze(dim=1))\n",
    "\n",
    "            # td_error = q_values - target.detach()\n",
    "            # sample['td_error'] = td_error.abs().detach()\n",
    "            \n",
    "            # loss = torch.mean((td_error * sample['_weight'])**2)\n",
    "            # # loss = torch.mean((td_error)**2)\n",
    "            # self.tdrb.update_tensordict_priority(sample)\n",
    "            \n",
    "            # loss.backward()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fcff5-a260-49fa-8607-37747e88f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3570/379586011.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'state': torch.tensor(self.states, dtype=torch.float32),\n",
      "/tmp/ipykernel_3570/379586011.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'hidden_state': torch.tensor(self.hidden_states, dtype=torch.float32),\n",
      "/tmp/ipykernel_3570/379586011.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'next_state': torch.tensor(self.next_states, dtype=torch.float32),\n",
      "/tmp/ipykernel_3570/379586011.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'next_hidden_state': torch.tensor(self.next_hidden_states, dtype=torch.float32),\n",
      "/tmp/ipykernel_3570/379586011.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'done': torch.tensor(self.dones, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  mean last ten: 14.0\n",
      "episode: 10  mean last ten: 22.0\n",
      "episode: 20  mean last ten: 23.5\n",
      "episode: 30  mean last ten: 64.3\n",
      "episode: 40  mean last ten: 31.0\n",
      "episode: 50  mean last ten: 36.4\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([11, 1])\n",
      "burned_targets: torch.Size([11, 1])\n",
      "q_values: tensor([[10.2239],\n",
      "        [10.2328],\n",
      "        [10.2042],\n",
      "        [10.3087],\n",
      "        [10.1667],\n",
      "        [10.2913],\n",
      "        [10.1946],\n",
      "        [10.2354],\n",
      "        [10.2769],\n",
      "        [10.1899],\n",
      "        [10.1933]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.0027236968744546175\n",
      "tensor([[ 2.5224e-04,  3.7468e-05, -3.6812e-05,  ..., -6.6911e-05,\n",
      "          8.6385e-04, -4.9951e-04],\n",
      "        [-4.4286e-05, -2.7321e-06,  3.4160e-05,  ...,  3.1363e-05,\n",
      "         -1.8470e-04,  1.1508e-04],\n",
      "        [ 2.7475e-05,  1.4944e-06, -2.2990e-05,  ..., -2.1592e-05,\n",
      "          1.1111e-04, -7.0012e-05],\n",
      "        ...,\n",
      "        [-1.9583e-04, -3.1220e-05,  1.1703e-05,  ...,  4.0141e-05,\n",
      "         -6.5182e-04,  3.7283e-04],\n",
      "        [-2.2070e-06, -2.0695e-06, -1.6617e-05,  ..., -1.0362e-05,\n",
      "          1.7777e-05, -1.3935e-05],\n",
      "        [-8.8928e-05, -1.1735e-05,  2.2885e-05,  ...,  2.9562e-05,\n",
      "         -3.1740e-04,  1.8577e-04]])\n",
      "episode: 60  mean last ten: 37.1\n",
      "episode: 70  mean last ten: 54.0\n",
      "tensor([9.5855, 8.0999])\n",
      "tensor([9.6112, 8.4009])\n",
      "episode: 80  mean last ten: 49.7\n",
      "episode: 90  mean last ten: 35.2\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.4848],\n",
      "        [9.7469],\n",
      "        [9.7095],\n",
      "        [9.6731],\n",
      "        [9.6315],\n",
      "        [9.5056],\n",
      "        [9.5229],\n",
      "        [9.5454],\n",
      "        [9.7264],\n",
      "        [9.7107],\n",
      "        [9.5823],\n",
      "        [9.6633],\n",
      "        [9.5358],\n",
      "        [9.6203],\n",
      "        [9.5856]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.006330625619739294\n",
      "tensor([[-1.5496e-04, -5.0699e-04,  5.5325e-04,  ...,  2.5689e-04,\n",
      "         -2.3899e-03,  5.6752e-04],\n",
      "        [ 2.3740e-06,  2.5011e-05, -2.5660e-05,  ..., -3.0212e-05,\n",
      "          9.3032e-05, -2.9345e-05],\n",
      "        [-5.8922e-06, -6.1329e-06,  6.9633e-06,  ..., -1.3397e-05,\n",
      "         -5.4301e-05,  4.2127e-06],\n",
      "        ...,\n",
      "        [ 1.4469e-04,  5.3307e-04, -5.6414e-04,  ..., -3.4755e-04,\n",
      "          2.3632e-03, -6.0067e-04],\n",
      "        [ 9.7967e-06,  8.1528e-06, -8.5664e-06,  ...,  2.6297e-05,\n",
      "          8.0335e-05, -3.7153e-06],\n",
      "        [ 4.6498e-05,  1.6285e-04, -1.7952e-04,  ..., -9.2800e-05,\n",
      "          7.6123e-04, -1.8478e-04]])\n",
      "episode: 100  mean last ten: 52.7\n",
      "episode: 110  mean last ten: 37.4\n",
      "episode: 120  mean last ten: 69.7\n",
      "episode: 130  mean last ten: 64.7\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.4577],\n",
      "        [9.3075],\n",
      "        [9.1572],\n",
      "        [9.4574],\n",
      "        [8.6500],\n",
      "        [8.5863],\n",
      "        [8.8132],\n",
      "        [9.5061],\n",
      "        [9.8985],\n",
      "        [8.9668],\n",
      "        [9.2871],\n",
      "        [9.5698],\n",
      "        [8.6186],\n",
      "        [9.0492],\n",
      "        [9.3877]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.11664640158414841\n",
      "tensor([[-0.0005, -0.0017, -0.0021,  ...,  0.0020, -0.0004, -0.0079],\n",
      "        [ 0.0013, -0.0055, -0.0033,  ...,  0.0106,  0.0014, -0.0190],\n",
      "        [-0.0003,  0.0019,  0.0014,  ..., -0.0032, -0.0002,  0.0063],\n",
      "        ...,\n",
      "        [ 0.0015, -0.0031, -0.0013,  ...,  0.0067,  0.0005, -0.0078],\n",
      "        [-0.0006,  0.0016,  0.0007,  ..., -0.0034, -0.0009,  0.0057],\n",
      "        [ 0.0012, -0.0029, -0.0009,  ...,  0.0064,  0.0018, -0.0093]])\n",
      "tensor([8.5063, 8.9094])\n",
      "tensor([9.2986, 9.3049])\n",
      "episode: 140  mean last ten: 73.7\n",
      "episode: 150  mean last ten: 53.8\n",
      "episode: 160  mean last ten: 73.9\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([9, 1])\n",
      "burned_targets: torch.Size([9, 1])\n",
      "q_values: tensor([[8.6557],\n",
      "        [8.0977],\n",
      "        [9.1699],\n",
      "        [9.6131],\n",
      "        [8.8819],\n",
      "        [7.7540],\n",
      "        [5.1529],\n",
      "        [2.1829],\n",
      "        [1.2041]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.24481551349163055\n",
      "tensor([[-2.1898e-03, -1.0298e-03, -5.7858e-06,  ...,  2.7762e-03,\n",
      "          2.3239e-02, -1.2004e-02],\n",
      "        [-2.3805e-03, -1.7756e-04, -1.3239e-04,  ...,  3.7977e-04,\n",
      "          1.0577e-02, -7.1160e-03],\n",
      "        [ 1.5642e-03,  4.8385e-03,  4.2559e-03,  ..., -5.6603e-03,\n",
      "         -2.9156e-04,  3.7628e-03],\n",
      "        ...,\n",
      "        [-4.7870e-03, -7.1379e-03, -6.6905e-03,  ...,  1.0529e-02,\n",
      "          4.6768e-03, -1.0341e-02],\n",
      "        [-2.8658e-04, -8.3918e-04, -7.5468e-04,  ...,  1.1449e-03,\n",
      "          1.6035e-04, -6.5549e-04],\n",
      "        [-1.4207e-03,  7.8771e-04,  3.8546e-04,  ..., -1.0915e-03,\n",
      "          2.4285e-03, -2.3461e-03]])\n",
      "episode: 170  mean last ten: 86.3\n",
      "tensor([10.0128,  9.8267])\n",
      "tensor([10.2590, 10.0221])\n",
      "episode: 180  mean last ten: 106.8\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([7, 1])\n",
      "burned_targets: torch.Size([7, 1])\n",
      "q_values: tensor([[7.2974],\n",
      "        [7.0279],\n",
      "        [6.7571],\n",
      "        [5.0450],\n",
      "        [3.0613],\n",
      "        [1.6828],\n",
      "        [1.2044]], grad_fn=<GatherBackward0>)\n",
      "loss: 1.2677215337753296\n",
      "tensor([[ 2.3540e-03,  1.1164e-03,  6.1193e-03,  ..., -2.7216e-04,\n",
      "          1.3613e-03, -4.6167e-03],\n",
      "        [-2.2746e-03, -3.6268e-03,  1.7660e-02,  ...,  9.1355e-04,\n",
      "         -7.2827e-03, -1.4190e-03],\n",
      "        [ 1.4592e-03,  2.2580e-03, -4.5808e-03,  ..., -1.5043e-03,\n",
      "          2.3429e-03,  1.0901e-03],\n",
      "        ...,\n",
      "        [-8.1855e-03, -9.2293e-03,  5.2938e-02,  ..., -3.0319e-03,\n",
      "         -2.1451e-02, -4.6382e-03],\n",
      "        [-8.3985e-04, -1.8094e-03, -9.5452e-03,  ...,  3.7913e-03,\n",
      "          1.0481e-03,  1.1268e-03],\n",
      "        [-1.0703e-03, -3.3264e-03, -5.9932e-03,  ...,  5.3659e-03,\n",
      "         -6.6231e-04,  2.0880e-05]])\n",
      "episode: 190  mean last ten: 67.9\n",
      "episode: 200  mean last ten: 108.8\n",
      "reward: tensor([1., 1., 1.])\n",
      "targets: torch.Size([3, 1])\n",
      "burned_targets: torch.Size([3, 1])\n",
      "q_values: tensor([[10.5175],\n",
      "        [10.3575],\n",
      "        [10.5424]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.01493482943624258\n",
      "tensor([[ 4.9019e-04,  1.5145e-03, -9.5819e-04,  ..., -1.6676e-03,\n",
      "          1.7374e-03,  8.2328e-04],\n",
      "        [-6.3408e-05, -1.8414e-04,  1.1503e-04,  ...,  2.0484e-04,\n",
      "         -2.1522e-04, -9.6854e-05],\n",
      "        [ 8.0381e-05,  1.1472e-04, -3.7335e-05,  ..., -1.2284e-04,\n",
      "          1.6364e-04,  3.5257e-05],\n",
      "        ...,\n",
      "        [-7.0198e-04, -2.0815e-03,  1.3135e-03,  ...,  2.3184e-03,\n",
      "         -2.4227e-03, -1.1035e-03],\n",
      "        [-7.3667e-04, -2.3986e-03,  1.5602e-03,  ...,  2.6594e-03,\n",
      "         -2.7295e-03, -1.3234e-03],\n",
      "        [-5.0029e-05, -2.6637e-04,  1.9225e-04,  ...,  2.8777e-04,\n",
      "         -2.7471e-04, -1.7053e-04]])\n",
      "tensor([2.7938, 4.6728])\n",
      "tensor([0.9214, 2.3197])\n",
      "episode: 210  mean last ten: 105.1\n",
      "episode: 220  mean last ten: 67.4\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[8.1100],\n",
      "        [8.4629],\n",
      "        [8.1072],\n",
      "        [7.8985],\n",
      "        [8.0158],\n",
      "        [8.9565],\n",
      "        [9.5186],\n",
      "        [9.5752],\n",
      "        [9.4859],\n",
      "        [9.3869],\n",
      "        [9.3750],\n",
      "        [9.3493],\n",
      "        [9.4120],\n",
      "        [9.4427],\n",
      "        [9.2194]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.16689299046993256\n",
      "tensor([[-3.0215e-06,  6.8838e-04,  1.0216e-03,  ..., -1.2931e-03,\n",
      "         -1.0501e-03,  1.0004e-03],\n",
      "        [ 9.7789e-04,  3.7687e-04, -4.5530e-04,  ...,  1.6509e-03,\n",
      "          3.1148e-03, -4.4499e-03],\n",
      "        [ 7.1995e-04,  9.6197e-04, -4.6037e-04,  ..., -1.8524e-03,\n",
      "         -4.6407e-04, -4.0495e-04],\n",
      "        ...,\n",
      "        [ 1.2761e-03, -5.9739e-04, -2.0426e-03,  ...,  4.6428e-03,\n",
      "          6.3114e-03, -7.8848e-03],\n",
      "        [-5.9557e-05,  2.4811e-05, -2.2760e-03,  ..., -1.9856e-03,\n",
      "          9.3569e-06,  1.1873e-03],\n",
      "        [ 6.6611e-04,  3.7870e-04, -1.0410e-03,  ...,  2.6246e-04,\n",
      "          1.1246e-03, -2.5817e-03]])\n",
      "episode: 230  mean last ten: 132.1\n",
      "tensor([9.4652, 8.9368])\n",
      "tensor([9.9825, 9.5804])\n",
      "episode: 240  mean last ten: 108.2\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([8, 1])\n",
      "burned_targets: torch.Size([8, 1])\n",
      "q_values: tensor([[ 9.4022],\n",
      "        [10.0649],\n",
      "        [ 8.8326],\n",
      "        [ 9.5306],\n",
      "        [ 5.9642],\n",
      "        [ 6.1685],\n",
      "        [ 2.9312],\n",
      "        [ 2.4828]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.6144703030586243\n",
      "tensor([[-2.4854e-03, -1.1797e-03,  3.2948e-03,  ...,  8.7821e-03,\n",
      "         -4.4437e-03,  5.9731e-03],\n",
      "        [-2.6949e-04, -8.7739e-04,  2.3930e-03,  ..., -3.5625e-04,\n",
      "          1.9684e-03, -8.2323e-03],\n",
      "        [ 1.6982e-03,  1.2708e-03,  4.2028e-03,  ...,  7.5200e-03,\n",
      "         -3.9211e-03,  1.1717e-02],\n",
      "        ...,\n",
      "        [-6.0032e-04, -2.9674e-03,  3.5515e-04,  ..., -2.5337e-04,\n",
      "          1.1450e-03, -5.6122e-03],\n",
      "        [-4.7209e-03, -5.1112e-03,  3.4466e-03,  ...,  1.7205e-03,\n",
      "          1.9003e-05, -1.2529e-02],\n",
      "        [-4.5178e-03, -3.8591e-03,  7.0631e-04,  ..., -7.1225e-04,\n",
      "          1.8608e-03, -1.4975e-02]])\n",
      "episode: 250  mean last ten: 145.3\n",
      "episode: 260  mean last ten: 71.0\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([13, 1])\n",
      "burned_targets: torch.Size([13, 1])\n",
      "q_values: tensor([[ 9.9078],\n",
      "        [ 9.9803],\n",
      "        [10.0760],\n",
      "        [ 9.7861],\n",
      "        [ 9.2792],\n",
      "        [ 7.3405],\n",
      "        [ 6.7190],\n",
      "        [ 6.1451],\n",
      "        [ 6.1941],\n",
      "        [ 5.5622],\n",
      "        [ 5.1454],\n",
      "        [ 5.1258],\n",
      "        [ 2.4464]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.8037118315696716\n",
      "tensor([[ 0.0191,  0.0095, -0.0069,  ..., -0.0188,  0.0045, -0.0294],\n",
      "        [ 0.0099,  0.0057, -0.0041,  ..., -0.0100, -0.0004, -0.0092],\n",
      "        [-0.0024, -0.0015,  0.0009,  ...,  0.0023,  0.0002,  0.0037],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0007,  0.0001,  ..., -0.0016,  0.0027,  0.0013],\n",
      "        [-0.0035, -0.0025,  0.0015,  ...,  0.0032,  0.0028,  0.0108],\n",
      "        [ 0.0192,  0.0108, -0.0065,  ..., -0.0178,  0.0013, -0.0238]])\n",
      "tensor([9.4352, 9.7023])\n",
      "tensor([9.6451, 9.6625])\n",
      "episode: 270  mean last ten: 101.7\n",
      "episode: 280  mean last ten: 93.9\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[ 9.9376],\n",
      "        [10.0528],\n",
      "        [10.2709],\n",
      "        [10.0706],\n",
      "        [10.1012],\n",
      "        [10.1607],\n",
      "        [10.3814],\n",
      "        [10.4103],\n",
      "        [10.2057],\n",
      "        [10.1498],\n",
      "        [10.3646],\n",
      "        [10.4103],\n",
      "        [10.2236],\n",
      "        [10.4374],\n",
      "        [10.2758]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.023358510807156563\n",
      "tensor([[-1.3671e-04,  2.8661e-04,  6.5293e-04,  ..., -8.0639e-04,\n",
      "         -1.9550e-05, -1.0570e-04],\n",
      "        [ 1.6122e-06, -1.6332e-04, -2.8848e-04,  ...,  4.5435e-04,\n",
      "          3.7755e-05,  3.6807e-05],\n",
      "        [-1.2922e-03, -9.4561e-04,  1.1190e-03,  ...,  3.8159e-03,\n",
      "          7.1047e-04, -8.0418e-04],\n",
      "        ...,\n",
      "        [-2.8404e-03, -3.1680e-03,  8.6849e-04,  ...,  1.1709e-02,\n",
      "          1.9024e-03, -1.6371e-03],\n",
      "        [-2.9007e-03, -4.0861e-03, -2.8444e-04,  ...,  1.4577e-02,\n",
      "          2.3977e-03, -1.5298e-03],\n",
      "        [-1.3269e-03, -9.6692e-04,  1.1689e-03,  ...,  3.9329e-03,\n",
      "          6.9693e-04, -8.4906e-04]])\n",
      "tensor([9.0400, 9.0371])\n",
      "tensor([9.1187, 9.2029])\n",
      "episode: 290  mean last ten: 148.9\n",
      "reward: tensor([1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([6, 1])\n",
      "burned_targets: torch.Size([6, 1])\n",
      "q_values: tensor([[10.2010],\n",
      "        [ 8.5968],\n",
      "        [ 8.7709],\n",
      "        [ 5.1992],\n",
      "        [ 3.1027],\n",
      "        [ 1.9248]], grad_fn=<GatherBackward0>)\n",
      "loss: 1.6386440992355347\n",
      "tensor([[-9.2053e-03,  1.1900e-02,  2.7447e-03,  ..., -1.8399e-02,\n",
      "         -2.1417e-02,  4.7309e-02],\n",
      "        [-4.5387e-03,  9.2353e-03,  1.9079e-03,  ..., -1.1979e-02,\n",
      "         -1.0067e-02,  2.0775e-02],\n",
      "        [ 7.2967e-04, -1.8368e-03, -7.4259e-05,  ...,  1.2724e-03,\n",
      "          5.6787e-03, -2.7873e-03],\n",
      "        ...,\n",
      "        [-3.8007e-03,  1.1286e-02,  1.3042e-03,  ..., -1.2216e-02,\n",
      "         -1.3674e-02,  1.6383e-02],\n",
      "        [-1.6486e-04,  2.2169e-03,  3.8402e-04,  ..., -2.0264e-03,\n",
      "         -3.7442e-05, -1.0891e-03],\n",
      "        [-5.5854e-03,  7.9249e-03,  1.8436e-03,  ..., -1.3330e-02,\n",
      "         -1.3655e-02,  2.5422e-02]])\n",
      "episode: 300  mean last ten: 129.0\n",
      "reward: tensor([1.])\n",
      "targets: torch.Size([1, 1])\n",
      "burned_targets: torch.Size([1, 1])\n",
      "q_values: tensor([[10.2288]], grad_fn=<GatherBackward0>)\n",
      "loss: 1.0042939720733557e-05\n",
      "tensor([[ 1.2311e-05,  3.3419e-05,  9.1621e-06,  ..., -2.1421e-05,\n",
      "          2.6339e-05,  1.9054e-05],\n",
      "        [-2.8206e-07, -7.6566e-07, -2.0991e-07,  ...,  4.9078e-07,\n",
      "         -6.0345e-07, -4.3653e-07],\n",
      "        [ 3.3068e-06,  8.9764e-06,  2.4609e-06,  ..., -5.7538e-06,\n",
      "          7.0747e-06,  5.1178e-06],\n",
      "        ...,\n",
      "        [-2.0809e-05, -5.6487e-05, -1.5486e-05,  ...,  3.6208e-05,\n",
      "         -4.4520e-05, -3.2206e-05],\n",
      "        [-3.6065e-05, -9.7901e-05, -2.6840e-05,  ...,  6.2753e-05,\n",
      "         -7.7160e-05, -5.5817e-05],\n",
      "        [-7.5844e-07, -2.0588e-06, -5.6443e-07,  ...,  1.3197e-06,\n",
      "         -1.6226e-06, -1.1738e-06]])\n",
      "tensor([9.5833, 9.7122])\n",
      "tensor([9.6437, 9.6006])\n",
      "episode: 310  mean last ten: 181.0\n",
      "episode: 320  mean last ten: 131.1\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.9636],\n",
      "        [9.9810],\n",
      "        [9.9458],\n",
      "        [9.9548],\n",
      "        [9.9093],\n",
      "        [9.8608],\n",
      "        [9.8417],\n",
      "        [9.8300],\n",
      "        [9.9078],\n",
      "        [9.9316],\n",
      "        [9.9204],\n",
      "        [9.8733],\n",
      "        [9.7584],\n",
      "        [9.8071],\n",
      "        [9.5087]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.006041861139237881\n",
      "tensor([[-3.2847e-04, -3.5209e-04,  2.9469e-04,  ...,  1.8553e-03,\n",
      "         -2.4532e-04, -3.9763e-04],\n",
      "        [ 5.7290e-05,  8.2709e-05, -8.4050e-05,  ..., -3.9644e-04,\n",
      "          2.9457e-05,  1.0505e-04],\n",
      "        [-1.8927e-04, -3.1589e-04,  4.1104e-04,  ...,  1.8335e-03,\n",
      "         -1.6186e-04, -4.0128e-04],\n",
      "        ...,\n",
      "        [ 3.5425e-04,  1.1537e-05,  6.0527e-04,  ...,  1.1182e-03,\n",
      "          8.2528e-05, -1.3387e-04],\n",
      "        [ 9.8673e-04,  6.6753e-04,  1.0435e-04,  ..., -2.2303e-03,\n",
      "          5.3723e-04,  5.9649e-04],\n",
      "        [-2.5964e-04, -3.6843e-04,  4.4299e-04,  ...,  2.2024e-03,\n",
      "         -2.3684e-04, -4.4623e-04]])\n",
      "episode: 330  mean last ten: 118.9\n",
      "tensor([9.6262, 9.8813])\n",
      "tensor([10.0194, 10.0385])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[10.2077],\n",
      "        [10.2270],\n",
      "        [10.3228],\n",
      "        [10.3020],\n",
      "        [10.2736],\n",
      "        [10.2429],\n",
      "        [10.2153],\n",
      "        [10.1448],\n",
      "        [10.1580],\n",
      "        [10.1695],\n",
      "        [10.1653],\n",
      "        [10.1845],\n",
      "        [10.1988],\n",
      "        [10.3059],\n",
      "        [10.2757]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.0033663222566246986\n",
      "tensor([[ 7.2896e-05,  4.8899e-05,  1.4453e-04,  ..., -2.4222e-05,\n",
      "          1.1048e-04, -1.3652e-04],\n",
      "        [-4.3653e-05, -3.7588e-05, -2.7985e-05,  ...,  8.1008e-05,\n",
      "         -1.2607e-04,  1.0439e-04],\n",
      "        [-6.6436e-05, -6.7749e-05,  3.0538e-05,  ...,  2.0075e-04,\n",
      "         -2.7178e-04,  1.9066e-04],\n",
      "        ...,\n",
      "        [-1.0094e-03, -9.0388e-04, -9.5392e-05,  ...,  2.4174e-03,\n",
      "         -3.3716e-03,  2.5841e-03],\n",
      "        [-1.1615e-03, -1.0073e-03, -4.9224e-04,  ...,  2.3249e-03,\n",
      "         -3.5456e-03,  2.8612e-03],\n",
      "        [ 1.1194e-05,  7.7568e-06, -9.3413e-06,  ..., -3.4332e-05,\n",
      "          3.5512e-05, -2.6917e-05]])\n",
      "episode: 340  mean last ten: 131.0\n",
      "episode: 350  mean last ten: 166.4\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[10.4231],\n",
      "        [10.3542],\n",
      "        [10.3153],\n",
      "        [10.3056],\n",
      "        [10.2923],\n",
      "        [10.3022],\n",
      "        [10.2819],\n",
      "        [10.2474],\n",
      "        [10.2402],\n",
      "        [10.2455],\n",
      "        [10.2717],\n",
      "        [10.2743],\n",
      "        [10.2691],\n",
      "        [10.2534],\n",
      "        [10.2888]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.00832432322204113\n",
      "tensor([[ 1.1820e-04,  1.1281e-04,  3.0134e-05,  ..., -3.2984e-04,\n",
      "          4.2840e-04, -7.9896e-05],\n",
      "        [-2.1097e-05, -2.0300e-05,  8.0297e-08,  ...,  7.0990e-05,\n",
      "         -8.6531e-05,  1.6807e-05],\n",
      "        [ 1.5284e-04,  1.7150e-04,  4.0048e-05,  ..., -6.2474e-04,\n",
      "          7.5448e-04, -1.5390e-04],\n",
      "        ...,\n",
      "        [ 1.9986e-04,  3.3852e-04, -1.6995e-05,  ..., -1.7059e-03,\n",
      "          1.8455e-03, -4.3886e-04],\n",
      "        [ 8.3104e-06,  1.9836e-04, -6.0908e-05,  ..., -1.5445e-03,\n",
      "          1.5291e-03, -3.8652e-04],\n",
      "        [ 8.1008e-05,  7.5752e-05,  3.9875e-05,  ..., -1.9597e-04,\n",
      "          2.8322e-04, -4.8215e-05]])\n",
      "tensor([10.0909, 10.0896])\n",
      "tensor([ 9.9606, 10.1028])\n",
      "episode: 360  mean last ten: 155.3\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([13, 1])\n",
      "burned_targets: torch.Size([13, 1])\n",
      "q_values: tensor([[9.6181],\n",
      "        [9.6760],\n",
      "        [9.7245],\n",
      "        [9.8104],\n",
      "        [9.8446],\n",
      "        [9.8183],\n",
      "        [9.8075],\n",
      "        [9.7715],\n",
      "        [9.8054],\n",
      "        [9.7890],\n",
      "        [9.8286],\n",
      "        [9.7995],\n",
      "        [9.8510]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.0034007455687969923\n",
      "tensor([[-1.1678e-04, -8.6772e-05, -1.3022e-04,  ...,  2.0757e-04,\n",
      "         -3.3402e-04,  1.7375e-04],\n",
      "        [ 5.0853e-05,  2.8693e-05,  3.8297e-05,  ..., -1.0438e-04,\n",
      "          1.5726e-04, -8.6963e-05],\n",
      "        [-3.4100e-05, -3.2569e-05, -4.7446e-05,  ...,  8.7791e-05,\n",
      "         -1.3384e-04,  6.8403e-05],\n",
      "        ...,\n",
      "        [-4.9327e-05, -1.0066e-04, -1.7800e-04,  ...,  8.9222e-06,\n",
      "         -1.3170e-04, -7.5169e-05],\n",
      "        [ 2.3258e-06, -1.0985e-04, -2.1769e-04,  ..., -1.6545e-04,\n",
      "          4.8825e-05, -2.8679e-04],\n",
      "        [ 4.1402e-05,  2.9955e-05,  4.2795e-05,  ..., -7.6167e-05,\n",
      "          1.2457e-04, -5.5269e-05]])\n",
      "episode: 370  mean last ten: 136.8\n",
      "tensor([9.1095, 9.2370])\n",
      "tensor([9.2397, 9.2699])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[8.8134],\n",
      "        [8.9877],\n",
      "        [8.9379],\n",
      "        [8.7554],\n",
      "        [8.6457],\n",
      "        [8.6060],\n",
      "        [8.7408],\n",
      "        [8.8814],\n",
      "        [8.9905],\n",
      "        [9.2119],\n",
      "        [9.2446],\n",
      "        [9.1095],\n",
      "        [8.9800],\n",
      "        [8.7608],\n",
      "        [8.7717]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.06949157267808914\n",
      "tensor([[-1.0800e-03,  4.4702e-04,  2.9967e-03,  ..., -2.3876e-03,\n",
      "         -5.0671e-04,  8.1989e-05],\n",
      "        [ 2.0497e-04, -3.9370e-04, -1.4151e-03,  ...,  1.8865e-03,\n",
      "         -2.8088e-04,  1.1627e-03],\n",
      "        [ 8.7354e-05, -2.9126e-04, -8.3839e-04,  ...,  1.5480e-03,\n",
      "         -2.4858e-04,  1.0429e-03],\n",
      "        ...,\n",
      "        [ 1.7252e-03, -7.5307e-04,  2.1214e-04,  ..., -1.5414e-03,\n",
      "          2.6781e-03, -7.2056e-03],\n",
      "        [ 4.2364e-03, -1.5855e-03, -2.2827e-03,  ..., -1.5007e-03,\n",
      "          5.3941e-03, -1.3747e-02],\n",
      "        [-8.8474e-04, -9.5336e-05, -5.5058e-04,  ...,  2.1249e-03,\n",
      "         -1.5224e-03,  4.2155e-03]])\n",
      "episode: 380  mean last ten: 168.5\n",
      "episode: 390  mean last ten: 131.2\n",
      "reward: tensor([1., 1., 1., 1.])\n",
      "targets: torch.Size([4, 1])\n",
      "burned_targets: torch.Size([4, 1])\n",
      "q_values: tensor([[9.5920],\n",
      "        [9.6489],\n",
      "        [9.6848],\n",
      "        [9.6317]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.028062092140316963\n",
      "tensor([[ 5.3628e-04, -1.2359e-05,  1.3616e-04,  ..., -7.6851e-04,\n",
      "          4.9212e-04,  6.2269e-04],\n",
      "        [-2.6971e-04,  4.1455e-05, -8.3560e-05,  ...,  3.6144e-04,\n",
      "         -1.7165e-04, -4.3761e-04],\n",
      "        [ 3.5334e-04, -1.1780e-04,  9.7986e-05,  ..., -4.1696e-04,\n",
      "          6.9895e-05,  7.8780e-04],\n",
      "        ...,\n",
      "        [-3.0277e-04, -5.0172e-04, -3.1007e-04,  ...,  8.7998e-04,\n",
      "         -1.6126e-03,  1.3888e-03],\n",
      "        [-9.6900e-04, -5.7401e-04, -5.3876e-04,  ...,  1.9166e-03,\n",
      "         -2.4634e-03,  9.1099e-04],\n",
      "        [-1.3331e-04,  1.2034e-04,  1.1016e-05,  ...,  8.5722e-05,\n",
      "          1.7839e-04, -5.5222e-04]])\n",
      "tensor([9.6092, 9.5466])\n",
      "tensor([9.5772, 9.6170])\n",
      "episode: 400  mean last ten: 142.7\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.5541],\n",
      "        [9.6078],\n",
      "        [9.5901],\n",
      "        [9.5233],\n",
      "        [9.5409],\n",
      "        [9.5313],\n",
      "        [9.5458],\n",
      "        [9.5027],\n",
      "        [9.5017],\n",
      "        [9.5198],\n",
      "        [9.5728],\n",
      "        [9.5468],\n",
      "        [9.5134],\n",
      "        [9.4542],\n",
      "        [9.4672]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.002002888126298785\n",
      "tensor([[ 2.9787e-06,  3.3240e-06,  1.1143e-05,  ...,  3.8107e-05,\n",
      "         -1.2448e-05,  7.6499e-05],\n",
      "        [-1.0514e-04, -2.9430e-05,  8.6731e-05,  ...,  3.6381e-04,\n",
      "         -2.8282e-04,  7.7157e-04],\n",
      "        [-1.8985e-06, -1.1671e-06,  3.7612e-06,  ...,  1.2924e-05,\n",
      "          1.6189e-06, -6.3407e-06],\n",
      "        ...,\n",
      "        [-1.5422e-05, -1.5104e-05, -3.4335e-05,  ..., -3.0727e-04,\n",
      "          8.6035e-05, -5.9032e-04],\n",
      "        [-4.2450e-05, -2.7975e-05, -4.6205e-05,  ..., -3.4575e-04,\n",
      "          6.9207e-05, -6.5079e-04],\n",
      "        [-4.0034e-05, -1.1421e-05,  4.3047e-05,  ...,  1.7015e-04,\n",
      "         -1.0307e-04,  2.9942e-04]])\n",
      "episode: 410  mean last ten: 119.7\n",
      "tensor([9.7592, 9.8244])\n",
      "tensor([9.8686, 9.8409])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.2721],\n",
      "        [9.2935],\n",
      "        [9.2306],\n",
      "        [9.3284],\n",
      "        [9.3064],\n",
      "        [9.3460],\n",
      "        [9.3911],\n",
      "        [9.4225],\n",
      "        [9.4904],\n",
      "        [9.3344],\n",
      "        [9.3244],\n",
      "        [9.3278],\n",
      "        [9.3205],\n",
      "        [9.4074],\n",
      "        [9.4161]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.19293563067913055\n",
      "tensor([[ 2.6519e-04,  3.2939e-05,  1.1784e-03,  ..., -6.2625e-04,\n",
      "          2.7938e-05, -6.1730e-04],\n",
      "        [-2.7141e-04, -1.4259e-05, -7.1366e-04,  ...,  1.0759e-03,\n",
      "          1.6949e-04,  4.6173e-04],\n",
      "        [ 1.3616e-04,  2.2457e-05,  5.7165e-04,  ..., -2.9266e-04,\n",
      "          5.6489e-05, -2.7837e-04],\n",
      "        ...,\n",
      "        [ 8.7384e-04,  2.5865e-04,  5.8190e-03,  ..., -2.6617e-03,\n",
      "         -1.6201e-04, -2.9713e-03],\n",
      "        [ 5.9964e-04,  3.6863e-04,  7.3957e-03,  ..., -2.2082e-03,\n",
      "         -6.2372e-04, -3.1923e-03],\n",
      "        [-3.0950e-04, -1.0182e-04, -2.4599e-03,  ...,  1.4443e-03,\n",
      "          4.8728e-04,  8.5268e-04]])\n",
      "episode: 420  mean last ten: 310.4\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.1634],\n",
      "        [9.1527],\n",
      "        [9.3944],\n",
      "        [9.4633],\n",
      "        [9.2758],\n",
      "        [9.6215],\n",
      "        [9.7163],\n",
      "        [9.6999],\n",
      "        [9.6409],\n",
      "        [9.5985],\n",
      "        [9.6961],\n",
      "        [9.5281],\n",
      "        [9.5321],\n",
      "        [9.5320],\n",
      "        [9.6341]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.025751452893018723\n",
      "tensor([[ 1.3251e-04, -8.0374e-05, -4.3379e-04,  ..., -6.4261e-05,\n",
      "          9.0875e-04, -2.2396e-03],\n",
      "        [-8.0995e-05,  5.1617e-05,  2.4697e-04,  ...,  5.2904e-05,\n",
      "         -4.8740e-04,  1.2078e-03],\n",
      "        [ 1.2289e-05,  7.2059e-06,  3.2947e-05,  ..., -1.0329e-05,\n",
      "         -9.8875e-06, -1.5762e-05],\n",
      "        ...,\n",
      "        [-6.7863e-04,  7.4261e-05,  1.6284e-03,  ...,  2.7747e-04,\n",
      "         -4.1327e-03,  1.0288e-02],\n",
      "        [-3.8832e-04, -4.4557e-06,  6.9877e-04,  ..., -1.8057e-04,\n",
      "         -2.8144e-03,  7.5324e-03],\n",
      "        [ 9.5721e-05,  3.0201e-05,  1.4335e-04,  ..., -9.0437e-05,\n",
      "          1.1034e-04, -3.2290e-04]])\n",
      "tensor([9.2319, 9.5573])\n",
      "tensor([9.6157, 9.6546])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[ 9.8220],\n",
      "        [ 9.6312],\n",
      "        [ 9.8654],\n",
      "        [ 9.9269],\n",
      "        [ 9.8875],\n",
      "        [ 9.9114],\n",
      "        [ 9.9479],\n",
      "        [ 9.9068],\n",
      "        [ 9.9334],\n",
      "        [ 9.9696],\n",
      "        [ 9.9886],\n",
      "        [ 9.9697],\n",
      "        [ 9.9590],\n",
      "        [10.0059],\n",
      "        [ 9.9983]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.00487219775095582\n",
      "tensor([[ 4.8151e-05, -2.1730e-06, -3.1530e-05,  ..., -6.2573e-05,\n",
      "          3.0270e-04, -8.7973e-04],\n",
      "        [-1.5700e-05, -3.7154e-06, -1.2906e-05,  ...,  1.7650e-05,\n",
      "         -8.8161e-05,  2.5858e-04],\n",
      "        [ 8.9098e-06,  2.8563e-06,  1.4905e-05,  ..., -9.1144e-06,\n",
      "          5.9538e-05, -1.9192e-04],\n",
      "        ...,\n",
      "        [-1.5131e-04,  5.2516e-05,  3.8078e-04,  ...,  2.6725e-04,\n",
      "         -1.1729e-03,  2.9410e-03],\n",
      "        [-1.3822e-05,  7.7285e-05,  3.6509e-04,  ...,  1.3115e-04,\n",
      "         -3.0514e-04,  1.8641e-04],\n",
      "        [ 1.5949e-05, -1.0086e-05, -6.6398e-05,  ..., -3.1939e-05,\n",
      "          1.3088e-04, -3.1980e-04]])\n",
      "episode: 430  mean last ten: 329.2\n",
      "tensor([8.6406, 8.7583])\n",
      "tensor([8.6487, 8.5835])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.6734],\n",
      "        [9.8016],\n",
      "        [9.8423],\n",
      "        [9.8332],\n",
      "        [9.8124],\n",
      "        [9.7735],\n",
      "        [9.8240],\n",
      "        [9.7613],\n",
      "        [9.8409],\n",
      "        [9.7745],\n",
      "        [9.8743],\n",
      "        [9.9011],\n",
      "        [9.9072],\n",
      "        [9.8893],\n",
      "        [9.8545]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.006322769448161125\n",
      "tensor([[-1.1636e-05,  1.6362e-06,  1.9689e-05,  ...,  4.6253e-05,\n",
      "         -8.7959e-05,  1.7064e-04],\n",
      "        [ 4.9827e-05,  2.9778e-06,  1.0583e-04,  ..., -7.8481e-05,\n",
      "          2.9327e-04, -5.9515e-04],\n",
      "        [-3.9651e-05, -4.3823e-06, -1.2465e-04,  ...,  3.6707e-05,\n",
      "         -2.0895e-04,  4.3918e-04],\n",
      "        ...,\n",
      "        [-5.7209e-05, -2.7456e-05, -6.0902e-04,  ..., -1.8237e-04,\n",
      "         -1.0232e-04,  3.0778e-04],\n",
      "        [ 7.0746e-05, -1.2917e-05, -2.6639e-04,  ..., -3.5029e-04,\n",
      "          6.1568e-04, -1.1915e-03],\n",
      "        [ 4.0024e-05,  7.4259e-06,  2.0936e-04,  ..., -1.5709e-06,\n",
      "          1.7110e-04, -4.1894e-04]])\n",
      "episode: 440  mean last ten: 254.1\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.0682],\n",
      "        [9.7013],\n",
      "        [9.8419],\n",
      "        [9.8578],\n",
      "        [9.6646],\n",
      "        [9.8080],\n",
      "        [9.6622],\n",
      "        [9.7917],\n",
      "        [9.7875],\n",
      "        [9.7742],\n",
      "        [9.7692],\n",
      "        [9.7819],\n",
      "        [9.7717],\n",
      "        [9.6240],\n",
      "        [9.5434]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.029694313183426857\n",
      "tensor([[-1.5067e-04,  1.6542e-05, -2.5560e-05,  ..., -2.7708e-05,\n",
      "          2.7163e-04, -1.7729e-04],\n",
      "        [-2.5381e-05,  8.7595e-05, -3.6843e-04,  ...,  2.7371e-03,\n",
      "          1.7766e-03, -1.2458e-03],\n",
      "        [-1.3336e-04, -9.3939e-05,  5.2181e-04,  ..., -3.4904e-03,\n",
      "         -2.1827e-03,  1.6131e-03],\n",
      "        ...,\n",
      "        [-1.4407e-03,  2.0933e-04, -1.3944e-03,  ...,  5.2448e-03,\n",
      "          3.6341e-03, -2.4532e-03],\n",
      "        [-1.2572e-03,  3.2802e-04, -2.6674e-03,  ...,  1.0968e-02,\n",
      "          7.0922e-03, -5.2681e-03],\n",
      "        [ 1.5037e-04, -1.1838e-05,  2.3033e-04,  ..., -3.8690e-04,\n",
      "         -3.6801e-04,  3.1093e-04]])\n",
      "tensor([8.7280, 6.9618])\n",
      "tensor([8.1123, 6.7139])\n",
      "episode: 450  mean last ten: 231.3\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.5101],\n",
      "        [9.3640],\n",
      "        [9.5774],\n",
      "        [9.6562],\n",
      "        [9.6697],\n",
      "        [9.6534],\n",
      "        [9.5864],\n",
      "        [9.6629],\n",
      "        [9.6609],\n",
      "        [9.6667],\n",
      "        [9.6594],\n",
      "        [9.6501],\n",
      "        [9.6197],\n",
      "        [9.4957],\n",
      "        [8.6528]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.02261776477098465\n",
      "tensor([[ 8.2742e-05,  7.5496e-05, -1.1502e-04,  ..., -1.3611e-04,\n",
      "          1.9345e-04,  1.2703e-04],\n",
      "        [-1.9394e-04, -1.3914e-04, -1.9796e-04,  ...,  1.9379e-04,\n",
      "         -2.1729e-04, -3.2992e-05],\n",
      "        [ 1.0070e-04,  6.4112e-05, -3.1643e-05,  ..., -9.6354e-05,\n",
      "          1.4539e-04,  6.8136e-05],\n",
      "        ...,\n",
      "        [-1.2285e-03, -9.5439e-04, -2.0112e-03,  ...,  9.8212e-04,\n",
      "         -1.0698e-03,  4.3150e-04],\n",
      "        [-1.6707e-03, -1.1915e-03, -1.5158e-03,  ...,  1.4037e-03,\n",
      "         -1.7909e-03,  1.1184e-04],\n",
      "        [ 1.9921e-05, -1.3082e-05,  1.2282e-04,  ...,  5.9612e-05,\n",
      "         -5.2016e-05, -1.1677e-04]])\n",
      "tensor([9.6510, 9.6559])\n",
      "tensor([9.5989, 9.5953])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([10, 1])\n",
      "burned_targets: torch.Size([10, 1])\n",
      "q_values: tensor([[9.7610],\n",
      "        [9.6517],\n",
      "        [9.9875],\n",
      "        [9.4900],\n",
      "        [9.9872],\n",
      "        [8.3823],\n",
      "        [3.4302],\n",
      "        [3.7387],\n",
      "        [1.8456],\n",
      "        [1.6909]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.15873341262340546\n",
      "tensor([[-1.0796e-03,  5.7755e-04, -1.6025e-03,  ..., -1.1696e-03,\n",
      "         -3.7516e-04, -3.3595e-03],\n",
      "        [ 2.8009e-04, -2.1862e-04,  5.7605e-04,  ...,  5.7325e-04,\n",
      "          7.2500e-05,  1.2593e-03],\n",
      "        [ 1.8731e-04, -1.9706e-03,  1.6130e-03,  ...,  6.5670e-03,\n",
      "         -4.4713e-04,  5.4063e-03],\n",
      "        ...,\n",
      "        [ 3.3842e-05, -5.5057e-05, -3.1986e-03,  ..., -1.9744e-03,\n",
      "          3.7197e-04, -4.7405e-03],\n",
      "        [-3.2947e-03,  1.6403e-03, -9.0459e-03,  ..., -6.0240e-03,\n",
      "         -8.1355e-04, -1.6094e-02],\n",
      "        [-8.7324e-04,  3.4791e-04, -1.1551e-03,  ..., -5.6681e-04,\n",
      "         -3.6010e-04, -2.2164e-03]])\n",
      "episode: 460  mean last ten: 334.4\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[8.0557],\n",
      "        [4.1935],\n",
      "        [6.7894],\n",
      "        [5.6878],\n",
      "        [6.4861],\n",
      "        [4.9741],\n",
      "        [4.9307],\n",
      "        [5.4049],\n",
      "        [6.5736],\n",
      "        [7.6791],\n",
      "        [8.4424],\n",
      "        [8.9526],\n",
      "        [9.3588],\n",
      "        [9.6704],\n",
      "        [9.5918]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.4302752912044525\n",
      "tensor([[-1.0631e-04, -6.2821e-04,  1.3236e-03,  ...,  3.5474e-04,\n",
      "          4.5978e-04,  5.1439e-03],\n",
      "        [-3.0538e-04, -8.1082e-04, -6.2329e-04,  ..., -6.0631e-04,\n",
      "         -6.2556e-03,  4.0056e-02],\n",
      "        [ 9.7070e-04,  6.0609e-04,  5.0054e-03,  ...,  2.3764e-03,\n",
      "          2.0221e-02, -8.1637e-02],\n",
      "        ...,\n",
      "        [-2.2089e-03, -4.3704e-03, -1.4633e-02,  ..., -4.4175e-03,\n",
      "         -4.7018e-02,  2.1460e-01],\n",
      "        [-4.3384e-04, -7.3120e-04, -1.8179e-03,  ..., -9.3602e-04,\n",
      "         -7.8308e-03,  4.4005e-02],\n",
      "        [-5.6792e-04, -5.9635e-04,  1.6525e-03,  ..., -8.1378e-04,\n",
      "         -7.5002e-03,  4.4906e-02]])\n",
      "tensor([9.7969, 9.8320])\n",
      "tensor([9.7945, 9.7733])\n",
      "episode: 470  mean last ten: 194.6\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[9.1010],\n",
      "        [9.1962],\n",
      "        [9.1458],\n",
      "        [9.0261],\n",
      "        [8.9219],\n",
      "        [8.7506],\n",
      "        [8.7278],\n",
      "        [8.7791],\n",
      "        [8.9287],\n",
      "        [8.9128],\n",
      "        [8.7782],\n",
      "        [8.7130],\n",
      "        [8.7449],\n",
      "        [8.8143],\n",
      "        [8.8201]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.023859303444623947\n",
      "tensor([[ 4.3971e-04, -8.3018e-05, -3.1021e-04,  ..., -1.4692e-03,\n",
      "         -2.6231e-04, -9.5989e-05],\n",
      "        [-8.4692e-04,  5.2526e-05,  7.6879e-04,  ...,  2.6858e-03,\n",
      "          3.8189e-04, -1.2487e-03],\n",
      "        [-9.3413e-05,  6.9676e-06,  1.7002e-04,  ...,  1.9965e-04,\n",
      "          4.4008e-06, -1.3317e-04],\n",
      "        ...,\n",
      "        [ 8.5006e-04, -9.4703e-05, -3.8220e-03,  ..., -1.8907e-03,\n",
      "          1.9684e-04, -1.8268e-04],\n",
      "        [-9.3163e-04, -1.7935e-05, -3.0126e-03,  ...,  3.8878e-03,\n",
      "          1.1028e-03, -3.7534e-03],\n",
      "        [-1.7707e-04, -2.8385e-05, -6.0325e-04,  ...,  1.1976e-03,\n",
      "          3.7231e-04, -3.4746e-04]])\n",
      "tensor([8.9664, 7.9336])\n",
      "tensor([6.7604, 6.6576])\n",
      "episode: 480  mean last ten: 238.6\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[7.9817],\n",
      "        [6.1329],\n",
      "        [8.1427],\n",
      "        [3.7196],\n",
      "        [4.0871],\n",
      "        [5.3345],\n",
      "        [7.6204],\n",
      "        [6.0832],\n",
      "        [6.2763],\n",
      "        [6.8768],\n",
      "        [6.5367],\n",
      "        [5.8332],\n",
      "        [5.8548],\n",
      "        [5.8322],\n",
      "        [6.8391]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.20695094764232635\n",
      "tensor([[-4.2910e-03,  7.2281e-05,  6.9279e-03,  ...,  1.2679e-02,\n",
      "          3.0626e-03, -4.0171e-03],\n",
      "        [ 8.5543e-04,  1.9637e-04, -3.6994e-03,  ..., -4.0675e-03,\n",
      "         -7.6259e-04,  4.3814e-03],\n",
      "        [ 2.2286e-03, -1.0806e-04, -9.3692e-04,  ..., -4.2413e-03,\n",
      "         -7.6421e-04,  3.1627e-03],\n",
      "        ...,\n",
      "        [-1.1035e-02,  2.7092e-03,  1.1725e-02,  ...,  3.2975e-02,\n",
      "          7.2426e-03, -1.7538e-02],\n",
      "        [-1.0891e-02,  3.2402e-03,  1.0727e-02,  ...,  9.0484e-03,\n",
      "         -9.7077e-05, -3.1059e-03],\n",
      "        [-1.5958e-04,  1.9492e-04,  2.5870e-03,  ..., -2.4176e-03,\n",
      "         -8.9507e-04, -4.4956e-04]])\n",
      "episode: 490  mean last ten: 244.8\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[ 9.9132],\n",
      "        [10.0921],\n",
      "        [10.0249],\n",
      "        [10.0692],\n",
      "        [10.0703],\n",
      "        [10.0308],\n",
      "        [10.0360],\n",
      "        [10.0622],\n",
      "        [10.0645],\n",
      "        [10.0765],\n",
      "        [ 9.9891],\n",
      "        [ 9.9491],\n",
      "        [ 9.9854],\n",
      "        [10.0656],\n",
      "        [10.0967]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.004398272838443518\n",
      "tensor([[ 6.2159e-05, -2.1206e-05, -1.3640e-04,  ..., -8.4999e-05,\n",
      "         -3.6089e-07, -1.2235e-05],\n",
      "        [-1.4161e-04,  5.3075e-05,  3.1693e-04,  ...,  2.3672e-04,\n",
      "          2.1419e-05,  2.6799e-05],\n",
      "        [ 1.1190e-04, -3.9976e-05, -1.8676e-04,  ..., -2.3063e-04,\n",
      "         -4.0446e-05, -1.0089e-05],\n",
      "        ...,\n",
      "        [-5.0930e-05, -4.3678e-05, -5.1662e-04,  ..., -1.7108e-04,\n",
      "         -8.7976e-05,  1.0262e-04],\n",
      "        [-3.7363e-04,  5.0629e-05,  1.1305e-04,  ...,  3.0349e-04,\n",
      "         -1.9880e-05,  7.8489e-05],\n",
      "        [-4.8050e-05,  1.6862e-05,  4.7352e-05,  ...,  1.1733e-04,\n",
      "          2.6669e-05,  1.1234e-06]])\n",
      "tensor([7.6431, 6.6224])\n",
      "tensor([7.3327, 6.6255])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([15, 1])\n",
      "burned_targets: torch.Size([15, 1])\n",
      "q_values: tensor([[10.1074],\n",
      "        [ 9.8792],\n",
      "        [ 9.9087],\n",
      "        [ 9.8560],\n",
      "        [ 9.8987],\n",
      "        [ 9.9508],\n",
      "        [10.0204],\n",
      "        [10.0105],\n",
      "        [10.0408],\n",
      "        [10.0455],\n",
      "        [10.0484],\n",
      "        [10.1093],\n",
      "        [10.2172],\n",
      "        [10.1639],\n",
      "        [10.1736]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.0024564049672335386\n",
      "tensor([[-3.6563e-06,  9.5401e-06,  9.9999e-05,  ...,  1.0542e-05,\n",
      "         -4.2009e-06, -2.8332e-05],\n",
      "        [-5.3472e-06,  1.2832e-05,  4.9797e-05,  ..., -2.3855e-06,\n",
      "         -7.5384e-06,  2.4967e-05],\n",
      "        [ 1.4887e-05, -8.1448e-06,  8.0451e-05,  ...,  7.5017e-06,\n",
      "          5.6451e-06, -1.1203e-04],\n",
      "        ...,\n",
      "        [ 3.3379e-04, -8.3097e-05,  5.4749e-04,  ..., -2.8201e-04,\n",
      "          4.9986e-04, -1.3287e-03],\n",
      "        [ 3.5590e-04, -8.5067e-05,  4.1928e-04,  ..., -3.1594e-04,\n",
      "          5.1453e-04, -1.3350e-03],\n",
      "        [-3.3489e-06, -7.9579e-06, -8.3901e-05,  ...,  3.5063e-06,\n",
      "         -1.1388e-05, -5.9186e-06]])\n",
      "episode: 500  mean last ten: 269.2\n",
      "tensor([9.8193, 9.8697])\n",
      "tensor([10.1655,  9.8785])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([12, 1])\n",
      "burned_targets: torch.Size([12, 1])\n",
      "q_values: tensor([[9.6254],\n",
      "        [9.6662],\n",
      "        [9.7168],\n",
      "        [9.8198],\n",
      "        [9.9253],\n",
      "        [9.8803],\n",
      "        [9.7034],\n",
      "        [9.8730],\n",
      "        [9.8886],\n",
      "        [9.9273],\n",
      "        [9.8988],\n",
      "        [9.8825]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.025785760954022408\n",
      "tensor([[ 1.6034e-04, -4.6812e-05, -1.8158e-04,  ..., -1.5227e-04,\n",
      "          1.0348e-04,  6.6423e-05],\n",
      "        [-6.4558e-05,  5.8228e-05,  3.6280e-04,  ...,  8.2961e-05,\n",
      "         -7.4411e-05,  1.3947e-04],\n",
      "        [ 4.4407e-05, -8.4239e-06, -6.3066e-05,  ..., -2.9760e-05,\n",
      "          2.6170e-05,  4.1082e-05],\n",
      "        ...,\n",
      "        [ 1.0862e-03,  4.7851e-04,  1.6749e-03,  ..., -8.1232e-04,\n",
      "          4.0139e-04,  2.2957e-03],\n",
      "        [ 1.1228e-03,  6.8725e-04,  3.0176e-03,  ..., -7.6253e-04,\n",
      "          2.9033e-04,  3.1169e-03],\n",
      "        [ 1.9451e-05,  3.8120e-05,  2.9790e-04,  ..., -6.6730e-07,\n",
      "          2.1268e-06,  2.3390e-04]])\n"
     ]
    }
   ],
   "source": [
    " # Environment parameters\n",
    "ENV = \"CartPole-v1\"\n",
    "env = gym.make('CartPole-v1')\n",
    "# убираем скорость из состояний\n",
    "# env = MaskVelocityWrapper(env)\n",
    "\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "episode_n = 1000\n",
    "\n",
    "agent = DRQNAgent(state_dim, action_dim, episode_n=episode_n)\n",
    "\n",
    "total_rewards = []\n",
    "loss1 = []\n",
    "grads1 = []\n",
    "loss2 = []\n",
    "grads2 = []\n",
    "counter = 0\n",
    "for episode in range(episode_n):\n",
    "\n",
    "    total_reward = 0\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        agent.add_sample(state, action, reward, next_state, terminated or truncated)\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 2 == 0:\n",
    "            agent.fit()\n",
    "    \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    \n",
    "        \n",
    "    total_rewards.append(total_reward)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"episode: {episode}  mean last ten: {np.mean(total_rewards[-10:])}\")\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ea6c970-b5b9-43ca-90d9-c26496ba5a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 4]).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9d122ca-0f32-45a9-9a75-2fe49e131fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1)\n",
    "input = torch.randn(1, 10)\n",
    "h0 = torch.randn(1, 20)\n",
    "c0 = torch.randn(1, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70787dd0-9de1-4bd6-b0ad-a8f567fdc42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1831,  0.2089,  0.4142, -0.0557, -0.0669,  0.3513, -0.1570, -0.2058,\n",
       "          0.1245,  0.1779,  0.2874, -0.1077, -0.2030, -0.0157, -0.0557,  0.1683,\n",
       "          0.0930,  0.0799, -0.3804,  0.1875]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "09077e12-3d59-4c1e-8256-ea329000b813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1831,  0.2089,  0.4142, -0.0557, -0.0669,  0.3513, -0.1570, -0.2058,\n",
       "          0.1245,  0.1779,  0.2874, -0.1077, -0.2030, -0.0157, -0.0557,  0.1683,\n",
       "          0.0930,  0.0799, -0.3804,  0.1875]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ff6f1269-dab1-4206-ab7b-d4f10a3d9066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3891,  0.4474,  1.2947, -0.1222, -0.1394,  1.9529, -0.2809, -0.3401,\n",
       "          0.3209,  0.8142,  0.6584, -0.2212, -0.4863, -0.0372, -0.0857,  0.6757,\n",
       "          0.2002,  0.1816, -0.9577,  0.4648]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "865664d4-4ce8-40aa-981e-7b155f531a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
