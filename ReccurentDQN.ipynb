{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e927c7b1-6575-4a8c-b5e8-01d88249ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import play \n",
    "from gym import wrappers\n",
    "from gym.wrappers import GrayScaleObservation, RecordEpisodeStatistics, TimeLimit, ResizeObservation, FrameStack\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hashlib import md5\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tensordict import TensorDict\n",
    "\n",
    "import torchrl\n",
    "from torchrl.data.replay_buffers import PrioritizedReplayBuffer, ReplayBuffer, PrioritizedSampler\n",
    "from torchrl.data import TensorDictPrioritizedReplayBuffer, LazyMemmapStorage, TensorDictReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe5c9fd-848a-42d6-b33e-c29e8d96e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskVelocityWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Gym environment observation wrapper used to mask velocity terms in\n",
    "    observations. The intention is the make the MDP partially observatiable.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(MaskVelocityWrapper, self).__init__(env)\n",
    "        if ENV == \"CartPole-v1\":\n",
    "            self.mask = np.array([1., 0., 1., 0.])\n",
    "        elif ENV == \"Pendulum-v0\":\n",
    "            self.mask = np.array([1., 1., 0.])\n",
    "        elif ENV == \"LunarLander-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        elif ENV == \"LunarLanderContinuous-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return  observation * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "30561323-8533-41c0-a7f1-a0f08707870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RQfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(state_dim, hidden_size=hidden_size, num_layers=1)\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.prev_hidden_state = None\n",
    "        self.hidden_state = None\n",
    "        self.current_batch_size = 0\n",
    "\n",
    "    def initialize(self, batch_size):\n",
    "        self.current_batch_size = batch_size\n",
    "        if batch_size == 0:\n",
    "            self.hidden_state = (torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size))\n",
    "        else:\n",
    "            self.hidden_state = (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))\n",
    "        self.prev_hidden_state = None\n",
    "\n",
    "    def forward(self, x, batch_size=None):\n",
    "        if len(x.shape) == 3:\n",
    "            batch_size = x.shape[0]\n",
    "        if len(x.shape) == 2:\n",
    "            batch_size = 0\n",
    "        if self.hidden_state is None or batch_size != self.current_batch_size:\n",
    "            self.initialize(batch_size)\n",
    "\n",
    "        self.prev_hidden_state = self.hidden_state\n",
    "        output, self.hidden_state = self.rnn(x, self.hidden_state)\n",
    "        q_values = self.q(output)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# class RecurrentActor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, hidden_size=64, batch_size=64):\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "64d81145-3912-42d0-ab8f-1305f1e068b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQNAgent(nn.Module):\n",
    "    def __init__(self, input_shape, action_n, lr=1e-3, gamma=0.95, batch_size=10, period=5, N=16, M=0, episode_n=1000):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.q_function = RQfunction(input_shape, action_n)\n",
    "        self.target_q_function = RQfunction(input_shape, action_n)\n",
    "        self.update_weights()\n",
    "\n",
    "        self.epsilon_min = 5e-3\n",
    "        self.epsilon_decay = 1.0 / (episode_n)\n",
    "        self.epsilon = 0.7\n",
    "\n",
    "        self.episode_n = episode_n\n",
    "        self.current_episode = 0\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.action_n = action_n\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.period = period\n",
    "        self.counter = 1\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[int(0.8 * self.episode_n)], gamma=0.5)\n",
    "\n",
    "        # self.capacity = 100_000\n",
    "        # storage = LazyMemmapStorage(self.capacity, scratch_dir='/home/artem/atari_games/tmp/')\n",
    "        # self.sampler = PrioritizedSampler(self.capacity, alpha=self.alpha, beta=self.beta)\n",
    "        # self.tdrb = TensorDictReplayBuffer(storage=storage, sampler=self.sampler, priority_key='td_error')\n",
    "        self.states_count = 0\n",
    "        self.rb = []\n",
    "\n",
    "        self.states = None\n",
    "        self.hidden_states =  None\n",
    "        self.actions = None\n",
    "        self.rewards = None\n",
    "        self.dones = None\n",
    "        self.next_states = None\n",
    "        self.next_hidden_states = None\n",
    "\n",
    "    def save_model(self, path=f'/home/artem/atari_games/models/DRQN_{md5(str(time.time()).encode()).hexdigest()}.pth'):\n",
    "        state = {\n",
    "            'model_dict': self.q_function.state_dict(),\n",
    "            'optimizer_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "\n",
    "    def load_model(self,path=f'/home/artem/atari_games/models/DRQN_{md5(str(time.time()).encode()).hexdigest()}'):\n",
    "        if os.path.exists(path):\n",
    "            state = torch.load(path)\n",
    "            self.optimizer.load_state_dict(state['optimizer_dict'])\n",
    "            self.q_function.load_state_dict(state['model_dict'])\n",
    "            self.target_q_function.load_state_dict(state['model_dict'])\n",
    "            self.epsilon = state['epsilon']\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max((self.epsilon - self.epsilon_decay), self.epsilon_min)\n",
    "\n",
    "    def decay_all(self):\n",
    "        self.decay_epsilon()\n",
    "\n",
    "    def e_greedy_action(self, q_values):\n",
    "        probs = np.ones(self.action_n) * self.epsilon / self.action_n\n",
    "        probs[np.argmax(q_values.numpy())] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_n), p=probs)\n",
    "        return action\n",
    "        \n",
    "    def get_action(self, obs: np.ndarray) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, dtype=torch.float).unsqueeze(dim=0)\n",
    "            q_values = self.q_function(obs).squeeze()\n",
    "            if self.counter % 1500 == 0:\n",
    "                print(q_values)\n",
    "            action = self.e_greedy_action(q_values)\n",
    "            return action\n",
    "\n",
    "    def add_sample(self, state, action, reward, next_state, done):\n",
    "        # print(f\"stacked {torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=0)}\")\n",
    "        # print(f\"hidden_states {0 if self.hidden_states is None else self.hidden_states.shape}\")\n",
    "\n",
    "        stacked_hidden_state = torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=0)\n",
    "        stacked_next_hidden_state = torch.stack((self.q_function.hidden_state[0], self.q_function.hidden_state[1]), dim=0)\n",
    "\n",
    "        self.states = torch.tensor(state).unsqueeze(dim=0) if self.states is None else torch.cat((self.states, torch.tensor(state).unsqueeze(dim=0)), dim=0)\n",
    "        # self.hidden_states.append(torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=1))\n",
    "        self.hidden_states = stacked_hidden_state.unsqueeze(dim=0) if self.hidden_states is None else torch.cat((self.hidden_states, stacked_hidden_state.unsqueeze(dim=0)), dim=0)\n",
    "        self.actions = torch.tensor(action).unsqueeze(dim=0) if self.actions is None else torch.cat((self.actions, torch.tensor(action).unsqueeze(dim=0)), dim=0)\n",
    "        self.rewards = torch.tensor(reward).unsqueeze(dim=0) if self.rewards is None else torch.cat((self.rewards, torch.tensor(reward).unsqueeze(dim=0)), dim=0)\n",
    "        self.dones = torch.tensor(int(done)).unsqueeze(dim=0) if self.dones is None else torch.cat((self.dones, torch.tensor(int(done)).unsqueeze(dim=0)), dim=0)\n",
    "        self.next_states = torch.tensor(next_state).unsqueeze(dim=0) if self.next_states is None else torch.cat((self.next_states, torch.tensor(next_state).unsqueeze(dim=0)), dim=0)\n",
    "        self.next_hidden_states = stacked_next_hidden_state.unsqueeze(dim=0) if self.next_hidden_states is None else torch.cat((self.next_hidden_states, stacked_next_hidden_state.unsqueeze(dim=0)), dim=0)\n",
    "        \n",
    "        self.states_count += 1\n",
    "\n",
    "        if done or self.states_count % self.N == 0:\n",
    "            # print(self.states.shape)\n",
    "            self.rb.append(\n",
    "                {\n",
    "                    'state': self.states,#.clone().detach().requires_grad_(False) ,\n",
    "                    'hidden_state': self.hidden_states.clone().detach().requires_grad_(False),\n",
    "                    'action': self.actions,\n",
    "                    'reward': self.rewards,\n",
    "                    'next_state': self.next_states,\n",
    "                    'next_hidden_state': self.next_hidden_states,\n",
    "                    'done': self.dones\n",
    "                        }\n",
    "            )\n",
    "\n",
    "            self.states = None\n",
    "            self.hidden_states =  None\n",
    "            self.actions = None\n",
    "            self.rewards = None\n",
    "            self.dones = None\n",
    "            self.next_states = None\n",
    "            self.next_hidden_states = None\n",
    "\n",
    "        if done:\n",
    "            self.current_episode += 1\n",
    "            self.decay_all()\n",
    "            self.q_function.prev_hidden_state = None\n",
    "            self.q_function.hidden_state = None\n",
    "\n",
    "    def update_weights(self):\n",
    "        for parameter_freeze, parameter in zip(self.target_q_function.rnn.parameters(), self.q_function.rnn.parameters()):\n",
    "            with torch.no_grad():\n",
    "                parameter_freeze.data.copy_(parameter.data)\n",
    "        for parameter_freeze, parameter in zip(self.target_q_function.q.parameters(), self.q_function.q.parameters()):\n",
    "            with torch.no_grad():\n",
    "                parameter_freeze.data.copy_(parameter.data)\n",
    "\n",
    "    def fit(self):\n",
    "        # print(len(self.rb))\n",
    "        if self.batch_size < len(self.rb):\n",
    "            # print('fit')\n",
    "            if self.counter % self.period == 0:\n",
    "                # print('weights change')\n",
    "                # не уверен, что LSTM можно так легко скопировать\n",
    "                self.update_weights()\n",
    "                \n",
    "            self.counter += 1\n",
    "\n",
    "            sample = random.sample(self.rb, self.batch_size)\n",
    "\n",
    "            flag = True\n",
    "            for rollout in sample:\n",
    "                # print(rollout['state'])\n",
    "                # вычисляем таргеты\n",
    "                self.q_function.hidden_state = (rollout['hidden_state'][0][0], rollout['hidden_state'][0][1])\n",
    "                self.target_q_function.hidden_state = (rollout['hidden_state'][0][0], rollout['hidden_state'][0][1])\n",
    "\n",
    "                # print(f\"q_func: {self.target_q_function(rollout['next_state'])}\")\n",
    "                targets = rollout['reward'].unsqueeze(dim=1) + (1 - rollout['done'].unsqueeze(dim=1)) * self.gamma * self.target_q_function(rollout['next_state'])\\\n",
    "                .gather(1, torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1))\n",
    "\n",
    "                \n",
    "                # print(f\"done: {(1 - rollout['done']).shape}\")\n",
    "                # print(f\"shape check: {torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1).shape}\")\n",
    "                # print(f\"target q: {self.target_q_function(rollout['next_state']).shape}\")\n",
    "                # print(f\"the whole: {self.target_q_function(rollout['next_state']).gather(1, torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1)).shape}\")\n",
    "                # print(f\"targets check: {targets.shape}\")\n",
    "                # вычисляем loss \n",
    "                q_values = self.q_function(rollout['state']).gather(1, rollout['action'].unsqueeze(dim=1))\n",
    "\n",
    "                # получаем последние  N - M состояний, так как их отображение q_values более правдоподобно из-за прогрева hidden_state\n",
    "                if targets.shape[0] <= self.M:\n",
    "                    burned_targets = targets\n",
    "                    burned_q_values = q_values\n",
    "                else:\n",
    "                    burned_targets = targets[self.M:]\n",
    "                    burned_q_values = q_values[self.M:]\n",
    "                \n",
    "                loss = torch.mean((burned_q_values - burned_targets.detach()) ** 2)\n",
    "                loss.backward()\n",
    "\n",
    "                # nn.utils.clip_grad_norm_(self.q_function.q.parameters(), max_norm=100.0)\n",
    "                # nn.utils.clip_grad_norm_(self.q_function.rnn.parameters(), max_norm=100.0)\n",
    "\n",
    "                if self.counter % 1000 == 0 and flag:\n",
    "                    # print(f\"target q: {self.target_q_function(rollout['next_state'])}\")\n",
    "                    print(f\"reward: {rollout['reward']}\")\n",
    "                    print(f\"targets: {targets.shape}\")\n",
    "                    print(f\"burned_targets: {burned_targets.shape}\")\n",
    "                    print(f\"q_values: {q_values}\")\n",
    "                    print(f\"loss: {loss}\")\n",
    "                    print(self.q_function.q[0].weight.grad)\n",
    "                    flag = False\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            self.scheduler.step()\n",
    "            # self.q_function.hidden_state = None\n",
    "            # self.target_q_function.hidden_state = None\n",
    "                \n",
    "            \n",
    "            # target = sample['reward'] + self.gamma * (1 - torch.tensor(sample['done'], dtype=torch.uint8)) * \\\n",
    "            # (self.target_q_function(sample['next_state'])[torch.arange(self.batch_size), torch.argmax(self.q_function(sample['next_state']), dim=1)])\n",
    "            # print(sample['next_state'].shape)\n",
    "            # print(self.target_q_function(sample['next_state']).shape)\n",
    "            # print(torch.argmax(self.q_function(sample['next_state']), dim=1).shape)\n",
    "            # print(sample['next_state'].shape)\n",
    "            # print(self.q_function(sample['next_state'].unsqueeze(dim=0) , self.batch_size).shape)\n",
    "            # print(torch.argmax(self.q_function(sample['next_state'].unsqueeze(dim=0), self.batch_size), dim=1).shape)\n",
    "            # target = sample['reward'] + self.gamma * (1 - torch.tensor(sample['done'], dtype=torch.uint8)) * \\\n",
    "            # (self.q_function(sample['next_state'].unsqueeze(dim=0) , self.batch_size).gather(1, torch.argmax(self.q_function(sample['next_state'].unsqueeze(dim=0), self.batch_size), dim=1).unsqueeze(dim=1)))\n",
    "            \n",
    "            # q_values = self.q_function(sample['state'].unsqueeze(dim=0), self.batch_size).gather(1, sample['action'].unsqueeze(dim=1))\n",
    "\n",
    "            # td_error = q_values - target.detach()\n",
    "            # sample['td_error'] = td_error.abs().detach()\n",
    "            \n",
    "            # loss = torch.mean((td_error * sample['_weight'])**2)\n",
    "            # # loss = torch.mean((td_error)**2)\n",
    "            # self.tdrb.update_tensordict_priority(sample)\n",
    "            \n",
    "            # loss.backward()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fcff5-a260-49fa-8607-37747e88f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  mean last ten: 18.0\n",
      "episode: 10  mean last ten: 18.1\n",
      "episode: 20  mean last ten: 31.9\n",
      "episode: 30  mean last ten: 32.0\n",
      "episode: 40  mean last ten: 23.4\n",
      "episode: 50  mean last ten: 27.3\n",
      "episode: 60  mean last ten: 27.7\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[21.1787],\n",
      "        [21.4114],\n",
      "        [21.6163],\n",
      "        [21.9463],\n",
      "        [21.7697],\n",
      "        [21.8990],\n",
      "        [21.9081],\n",
      "        [21.9186],\n",
      "        [21.8297],\n",
      "        [21.4082],\n",
      "        [21.8396],\n",
      "        [21.8995],\n",
      "        [20.6484],\n",
      "        [17.7867],\n",
      "        [13.2865],\n",
      "        [ 8.6898]], grad_fn=<GatherBackward0>)\n",
      "loss: 6.645094871520996\n",
      "tensor([[ 2.1062e-02,  2.4064e-02,  3.2734e-02,  ...,  3.1645e-02,\n",
      "         -8.8217e-02, -7.5861e-02],\n",
      "        [ 2.1572e-01,  5.8062e-01,  2.8930e-01,  ...,  3.2981e-01,\n",
      "         -1.0506e+00, -7.1112e-01],\n",
      "        [-4.8213e-02,  6.6106e-02, -1.0097e-01,  ..., -7.3784e-02,\n",
      "          1.6208e-01,  2.0526e-01],\n",
      "        ...,\n",
      "        [-8.7337e-03, -8.2956e-02,  2.1166e-02,  ..., -1.0990e-02,\n",
      "          3.7391e-02,  7.6883e-04],\n",
      "        [ 2.8927e-02,  1.1611e-01,  2.4801e-02,  ...,  4.2635e-02,\n",
      "         -1.4656e-01, -8.1443e-02],\n",
      "        [ 2.0312e-01,  4.4531e-01,  2.9111e-01,  ...,  3.0991e-01,\n",
      "         -9.5179e-01, -6.9326e-01]])\n",
      "episode: 70  mean last ten: 50.6\n",
      "episode: 80  mean last ten: 59.6\n",
      "tensor([19.0345, 18.7884])\n",
      "tensor([19.0858, 18.9736])\n",
      "episode: 90  mean last ten: 43.3\n",
      "episode: 100  mean last ten: 49.3\n",
      "episode: 110  mean last ten: 40.9\n",
      "reward: tensor([1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([6, 1])\n",
      "burned_targets: torch.Size([6, 1])\n",
      "q_values: tensor([[17.4197],\n",
      "        [16.9577],\n",
      "        [17.8784],\n",
      "        [16.7233],\n",
      "        [15.5455],\n",
      "        [12.8804]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.3268294036388397\n",
      "tensor([[ 4.7329e-03,  2.9489e-02, -4.4934e-04,  ..., -2.2918e-03,\n",
      "         -1.6251e-02,  5.2271e-04],\n",
      "        [ 1.3480e-04, -1.3911e-02,  1.4605e-02,  ..., -2.0368e-02,\n",
      "          2.3244e-02, -1.6415e-02],\n",
      "        [-2.5858e-03, -3.1104e-02,  1.8056e-02,  ..., -2.3826e-02,\n",
      "          3.6029e-02, -2.0256e-02],\n",
      "        ...,\n",
      "        [ 7.6347e-03,  1.1513e-01, -6.8891e-02,  ...,  9.1375e-02,\n",
      "         -1.3611e-01,  7.7519e-02],\n",
      "        [ 5.4929e-04,  7.9025e-03, -5.0290e-03,  ...,  6.6666e-03,\n",
      "         -9.6341e-03,  5.6853e-03],\n",
      "        [ 1.1881e-03, -4.9310e-04,  7.3733e-03,  ..., -1.0885e-02,\n",
      "          8.2673e-03, -8.4094e-03]])\n",
      "episode: 120  mean last ten: 38.4\n",
      "episode: 130  mean last ten: 50.6\n",
      "episode: 140  mean last ten: 31.5\n",
      "episode: 150  mean last ten: 48.0\n",
      "episode: 160  mean last ten: 31.0\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[16.3092],\n",
      "        [16.1724],\n",
      "        [16.5943],\n",
      "        [17.0085],\n",
      "        [16.7931],\n",
      "        [16.5151],\n",
      "        [16.8997],\n",
      "        [16.6136],\n",
      "        [16.3250],\n",
      "        [16.8943],\n",
      "        [16.5293],\n",
      "        [17.0748],\n",
      "        [17.3640],\n",
      "        [16.9312],\n",
      "        [17.4090],\n",
      "        [17.5835]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.3810412883758545\n",
      "tensor([[ 0.0059,  0.0142, -0.0109,  ...,  0.0140, -0.0248,  0.0100],\n",
      "        [-0.0100, -0.0207,  0.0149,  ..., -0.0206,  0.0361, -0.0141],\n",
      "        [-0.0158, -0.0345,  0.0257,  ..., -0.0356,  0.0614, -0.0244],\n",
      "        ...,\n",
      "        [ 0.0489,  0.1009, -0.0712,  ...,  0.0996, -0.1750,  0.0676],\n",
      "        [-0.0031, -0.0053,  0.0041,  ..., -0.0062,  0.0101, -0.0040],\n",
      "        [-0.0113, -0.0244,  0.0175,  ..., -0.0239,  0.0423, -0.0165]])\n",
      "tensor([13.0018, 13.3935])\n",
      "tensor([14.6819, 14.4560])\n",
      "episode: 170  mean last ten: 50.3\n",
      "episode: 180  mean last ten: 49.2\n",
      "episode: 190  mean last ten: 54.7\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[15.9406],\n",
      "        [13.3983],\n",
      "        [15.1809],\n",
      "        [15.0477],\n",
      "        [15.0666],\n",
      "        [14.8513],\n",
      "        [15.5786],\n",
      "        [15.7776],\n",
      "        [15.5118],\n",
      "        [14.7846],\n",
      "        [15.6019],\n",
      "        [15.9731],\n",
      "        [16.2814],\n",
      "        [15.0875],\n",
      "        [15.3799],\n",
      "        [13.0104]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.20320849120616913\n",
      "tensor([[-0.0047,  0.0088, -0.0047,  ...,  0.0131, -0.0312,  0.0046],\n",
      "        [ 0.0022,  0.0010, -0.0039,  ..., -0.0018, -0.0020,  0.0003],\n",
      "        [ 0.0040, -0.0023,  0.0016,  ..., -0.0076,  0.0144, -0.0020],\n",
      "        ...,\n",
      "        [-0.0109,  0.0046,  0.0046,  ...,  0.0184, -0.0267,  0.0040],\n",
      "        [ 0.0024,  0.0026, -0.0053,  ..., -0.0005, -0.0062,  0.0010],\n",
      "        [ 0.0022, -0.0005, -0.0018,  ..., -0.0032,  0.0033, -0.0006]])\n",
      "episode: 200  mean last ten: 67.7\n",
      "episode: 210  mean last ten: 60.2\n",
      "tensor([11.5375,  6.7996])\n",
      "tensor([16.2722, 14.4153])\n",
      "episode: 220  mean last ten: 91.2\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([12, 1])\n",
      "burned_targets: torch.Size([12, 1])\n",
      "q_values: tensor([[20.2311],\n",
      "        [19.8086],\n",
      "        [19.8213],\n",
      "        [19.9139],\n",
      "        [19.7600],\n",
      "        [19.1138],\n",
      "        [18.6387],\n",
      "        [18.0113],\n",
      "        [13.2473],\n",
      "        [ 6.2626],\n",
      "        [ 2.8024],\n",
      "        [ 0.9410]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.8241631984710693\n",
      "tensor([[-0.0081,  0.0029,  0.0020,  ...,  0.0095, -0.0053, -0.0083],\n",
      "        [ 0.0290, -0.0128,  0.0041,  ..., -0.0232,  0.0205,  0.0356],\n",
      "        [-0.0201,  0.0129, -0.0047,  ...,  0.0127, -0.0164, -0.0206],\n",
      "        ...,\n",
      "        [ 0.1131, -0.0469,  0.0120,  ..., -0.0904,  0.0786,  0.1419],\n",
      "        [ 0.0543, -0.0282,  0.0089,  ..., -0.0381,  0.0410,  0.0620],\n",
      "        [ 0.2853, -0.1264,  0.0356,  ..., -0.2409,  0.1998,  0.3483]])\n",
      "episode: 230  mean last ten: 71.3\n",
      "episode: 240  mean last ten: 92.9\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[15.3577],\n",
      "        [13.3509],\n",
      "        [14.7521],\n",
      "        [15.2803],\n",
      "        [15.6778],\n",
      "        [15.9728],\n",
      "        [16.1724],\n",
      "        [15.8302],\n",
      "        [15.9389],\n",
      "        [15.9496],\n",
      "        [16.2393],\n",
      "        [16.1410],\n",
      "        [16.1920],\n",
      "        [15.7594],\n",
      "        [16.2353],\n",
      "        [16.8228]], grad_fn=<GatherBackward0>)\n",
      "loss: 2.57810640335083\n",
      "tensor([[-8.9271e-03, -7.4399e-03, -5.4320e-04,  ..., -4.5466e-02,\n",
      "          9.2587e-03, -4.5344e-03],\n",
      "        [-1.6845e-03, -1.7475e-03, -1.0690e-04,  ..., -5.9030e-03,\n",
      "          1.2189e-03, -9.2072e-04],\n",
      "        [ 9.7362e-03,  9.2720e-03,  1.5030e-03,  ...,  4.6735e-02,\n",
      "         -9.1733e-03,  5.3364e-03],\n",
      "        ...,\n",
      "        [-2.7055e-02, -2.1805e-02, -1.1190e-03,  ..., -1.4114e-01,\n",
      "          2.9604e-02, -1.3448e-02],\n",
      "        [ 5.8980e-03,  5.0611e-03,  5.9976e-05,  ...,  3.0900e-02,\n",
      "         -7.1417e-03,  2.8822e-03],\n",
      "        [-4.2281e-03, -4.1460e-03, -3.2691e-04,  ..., -2.2825e-02,\n",
      "          5.1255e-03, -2.3835e-03]])\n",
      "tensor([16.1912, 14.9897])\n",
      "tensor([13.8103, 13.2226])\n",
      "episode: 250  mean last ten: 96.3\n",
      "episode: 260  mean last ten: 117.2\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[13.4253],\n",
      "        [11.5876],\n",
      "        [11.2586],\n",
      "        [12.0975],\n",
      "        [12.6585],\n",
      "        [14.0026],\n",
      "        [13.7902],\n",
      "        [11.5476],\n",
      "        [ 7.5071],\n",
      "        [ 8.8845],\n",
      "        [11.3343],\n",
      "        [ 8.4133],\n",
      "        [ 9.9066],\n",
      "        [11.3522],\n",
      "        [12.8671],\n",
      "        [12.1542]], grad_fn=<GatherBackward0>)\n",
      "loss: 1.7741641998291016\n",
      "tensor([[ 0.0019,  0.0044,  0.0029,  ...,  0.0236, -0.0034, -0.0008],\n",
      "        [ 0.0068,  0.0048,  0.0010,  ...,  0.0128, -0.0019,  0.0003],\n",
      "        [-0.0207, -0.0099,  0.0080,  ..., -0.0023,  0.0018, -0.0037],\n",
      "        ...,\n",
      "        [ 0.0373, -0.0023, -0.0047,  ..., -0.1222,  0.0168,  0.0163],\n",
      "        [-0.0077, -0.0011,  0.0029,  ...,  0.0155, -0.0018, -0.0023],\n",
      "        [ 0.0247,  0.0488, -0.0312,  ...,  0.2286, -0.0386, -0.0051]])\n",
      "episode: 270  mean last ten: 54.1\n",
      "tensor([11.3187,  9.2424])\n",
      "tensor([12.3483, 13.0800])\n",
      "episode: 280  mean last ten: 113.6\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[20.1562],\n",
      "        [19.9028],\n",
      "        [19.9714],\n",
      "        [20.1023],\n",
      "        [20.0681],\n",
      "        [19.9850],\n",
      "        [20.0087],\n",
      "        [20.0332],\n",
      "        [19.9523],\n",
      "        [20.2260],\n",
      "        [20.1727],\n",
      "        [19.9339],\n",
      "        [20.3160],\n",
      "        [20.3566],\n",
      "        [20.1665],\n",
      "        [19.9619]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.03915434330701828\n",
      "tensor([[-0.0004,  0.0011, -0.0010,  ...,  0.0033, -0.0006,  0.0006],\n",
      "        [ 0.0009, -0.0019,  0.0013,  ..., -0.0052,  0.0008, -0.0010],\n",
      "        [-0.0015,  0.0031, -0.0028,  ...,  0.0084, -0.0013,  0.0016],\n",
      "        ...,\n",
      "        [-0.0014,  0.0046, -0.0038,  ...,  0.0130, -0.0023,  0.0024],\n",
      "        [-0.0005,  0.0014, -0.0015,  ...,  0.0040, -0.0007,  0.0007],\n",
      "        [ 0.0007, -0.0008,  0.0005,  ..., -0.0019,  0.0001, -0.0005]])\n",
      "episode: 290  mean last ten: 152.0\n",
      "episode: 300  mean last ten: 125.6\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[16.0577],\n",
      "        [13.7476],\n",
      "        [17.2458],\n",
      "        [17.2085],\n",
      "        [17.4048],\n",
      "        [16.7669],\n",
      "        [17.2106],\n",
      "        [16.6394],\n",
      "        [17.1987],\n",
      "        [16.7112],\n",
      "        [17.2509],\n",
      "        [17.2641],\n",
      "        [17.0894],\n",
      "        [17.0833],\n",
      "        [17.0255],\n",
      "        [16.6084]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.6040502190589905\n",
      "tensor([[ 0.0085,  0.0052, -0.0076,  ...,  0.0083, -0.0024,  0.0032],\n",
      "        [ 0.0069,  0.0036, -0.0061,  ...,  0.0079, -0.0022,  0.0021],\n",
      "        [ 0.0008,  0.0009, -0.0020,  ...,  0.0028, -0.0003,  0.0005],\n",
      "        ...,\n",
      "        [ 0.0324,  0.0184, -0.0257,  ...,  0.0299, -0.0090,  0.0114],\n",
      "        [ 0.0028,  0.0016, -0.0048,  ...,  0.0057, -0.0008,  0.0011],\n",
      "        [ 0.0090,  0.0050, -0.0082,  ...,  0.0101, -0.0028,  0.0029]])\n",
      "tensor([12.9347, 14.6772])\n",
      "tensor([10.2004, 11.2564])\n",
      "episode: 310  mean last ten: 121.1\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[17.8907],\n",
      "        [17.1980],\n",
      "        [17.4444],\n",
      "        [16.1977],\n",
      "        [16.0805],\n",
      "        [16.2024],\n",
      "        [14.2086],\n",
      "        [14.5096],\n",
      "        [14.8468],\n",
      "        [15.5839],\n",
      "        [16.0406],\n",
      "        [16.3222],\n",
      "        [15.9237],\n",
      "        [16.4560],\n",
      "        [16.6575],\n",
      "        [16.8308]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.16313251852989197\n",
      "tensor([[-8.1906e-03, -9.0094e-03,  7.0795e-03,  ..., -3.1931e-02,\n",
      "          1.6981e-02,  4.2185e-03],\n",
      "        [-4.0068e-03, -4.4184e-03,  2.6673e-03,  ..., -1.8110e-02,\n",
      "          7.5333e-03,  1.1562e-03],\n",
      "        [ 1.4495e-02,  1.5955e-02, -1.2284e-02,  ...,  5.7362e-02,\n",
      "         -2.9208e-02, -6.9052e-03],\n",
      "        ...,\n",
      "        [-3.2144e-02, -3.4804e-02,  2.7254e-02,  ..., -1.2968e-01,\n",
      "          6.7512e-02,  1.7271e-02],\n",
      "        [-2.4864e-05,  2.6089e-04, -2.4745e-04,  ..., -4.4145e-04,\n",
      "          1.4717e-04,  1.5248e-04],\n",
      "        [-7.2338e-03, -7.6195e-03,  6.0542e-03,  ..., -3.0043e-02,\n",
      "          1.5839e-02,  4.4102e-03]])\n",
      "episode: 320  mean last ten: 113.5\n",
      "tensor([15.3904, 16.2374])\n",
      "tensor([14.1762, 15.7386])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([10, 1])\n",
      "burned_targets: torch.Size([10, 1])\n",
      "q_values: tensor([[22.9007],\n",
      "        [23.2080],\n",
      "        [23.1544],\n",
      "        [23.0987],\n",
      "        [22.7223],\n",
      "        [22.9187],\n",
      "        [23.1265],\n",
      "        [22.8932],\n",
      "        [20.1421],\n",
      "        [21.2429]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.23479315638542175\n",
      "tensor([[ 0.0061,  0.0134, -0.0072,  ...,  0.0077, -0.0036, -0.0008],\n",
      "        [ 0.0034,  0.0091, -0.0083,  ...,  0.0176, -0.0090, -0.0071],\n",
      "        [ 0.0037,  0.0099, -0.0088,  ...,  0.0197, -0.0096, -0.0073],\n",
      "        ...,\n",
      "        [-0.0066, -0.0167,  0.0128,  ..., -0.0243,  0.0119,  0.0083],\n",
      "        [ 0.0060,  0.0152, -0.0125,  ...,  0.0252, -0.0126, -0.0093],\n",
      "        [ 0.0129,  0.0304, -0.0141,  ...,  0.0165, -0.0056,  0.0015]])\n",
      "episode: 330  mean last ten: 209.8\n",
      "episode: 340  mean last ten: 163.1\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[17.6361],\n",
      "        [17.8261],\n",
      "        [17.6569],\n",
      "        [17.4684],\n",
      "        [17.7132],\n",
      "        [17.2861],\n",
      "        [17.3955],\n",
      "        [17.5587],\n",
      "        [17.1286],\n",
      "        [17.2242],\n",
      "        [17.5159],\n",
      "        [17.7523],\n",
      "        [17.5858],\n",
      "        [17.5796],\n",
      "        [17.5338],\n",
      "        [17.9038]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.04144817590713501\n",
      "tensor([[ 4.8467e-04,  1.8244e-05, -1.3268e-04,  ...,  8.4904e-04,\n",
      "          2.1147e-04,  4.2956e-05],\n",
      "        [ 3.0422e-04,  2.4198e-04, -3.7000e-04,  ...,  4.7679e-04,\n",
      "         -1.7824e-04,  4.7257e-05],\n",
      "        [ 3.3132e-04,  1.7430e-04, -2.2239e-04,  ...,  3.7334e-04,\n",
      "         -2.4608e-04,  1.0677e-05],\n",
      "        ...,\n",
      "        [-4.9493e-04, -1.0145e-03,  1.0728e-03,  ..., -5.7246e-04,\n",
      "          1.7769e-03, -8.1655e-05],\n",
      "        [ 6.2554e-04,  2.7402e-04, -3.7750e-04,  ...,  8.9205e-04,\n",
      "         -1.7332e-04,  4.3010e-05],\n",
      "        [ 1.2419e-04,  9.5283e-05, -1.0592e-04,  ...,  2.0876e-04,\n",
      "         -1.8009e-04,  1.5882e-05]])\n",
      "tensor([20.2596, 20.2931])\n",
      "tensor([20.1229, 20.2602])\n",
      "episode: 350  mean last ten: 188.2\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[22.3321],\n",
      "        [21.6699],\n",
      "        [22.5681],\n",
      "        [22.5449],\n",
      "        [22.3290],\n",
      "        [22.4764],\n",
      "        [22.2878],\n",
      "        [22.8197],\n",
      "        [22.7547],\n",
      "        [22.7194],\n",
      "        [22.7070],\n",
      "        [22.5993],\n",
      "        [22.4226],\n",
      "        [22.5882],\n",
      "        [22.4446],\n",
      "        [22.2480]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.17240378260612488\n",
      "tensor([[-0.0007, -0.0002,  0.0005,  ..., -0.0018,  0.0011, -0.0004],\n",
      "        [ 0.0003,  0.0015, -0.0014,  ...,  0.0022, -0.0022,  0.0003],\n",
      "        [-0.0089, -0.0142,  0.0188,  ..., -0.0221,  0.0159, -0.0047],\n",
      "        ...,\n",
      "        [-0.0062, -0.0109,  0.0137,  ..., -0.0186,  0.0148, -0.0037],\n",
      "        [-0.0088, -0.0137,  0.0183,  ..., -0.0216,  0.0157, -0.0046],\n",
      "        [ 0.0041,  0.0070, -0.0091,  ...,  0.0107, -0.0074,  0.0023]])\n",
      "tensor([18.1239, 18.5520])\n",
      "tensor([18.7306, 19.1710])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[18.5211],\n",
      "        [17.8300],\n",
      "        [18.9204],\n",
      "        [18.8460],\n",
      "        [19.0456],\n",
      "        [18.5083],\n",
      "        [18.0965],\n",
      "        [17.8210],\n",
      "        [18.5982],\n",
      "        [17.4838],\n",
      "        [18.4410],\n",
      "        [18.6106],\n",
      "        [19.0851],\n",
      "        [19.2755],\n",
      "        [19.5112],\n",
      "        [19.5713]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.09844677150249481\n",
      "tensor([[-8.4601e-03, -4.3057e-03,  1.6309e-02,  ..., -6.0478e-02,\n",
      "          4.9273e-02, -4.8437e-03],\n",
      "        [ 9.1717e-04,  4.0778e-04, -1.5895e-03,  ...,  6.6623e-03,\n",
      "         -5.1822e-03,  5.6613e-04],\n",
      "        [ 8.9317e-06, -1.2109e-05,  1.6614e-04,  ..., -4.3227e-04,\n",
      "          4.5940e-04, -7.4302e-06],\n",
      "        ...,\n",
      "        [-1.2421e-02, -5.9422e-03,  2.2997e-02,  ..., -8.8098e-02,\n",
      "          7.0478e-02, -6.9250e-03],\n",
      "        [-3.3620e-03, -1.5836e-03,  6.2180e-03,  ..., -2.4041e-02,\n",
      "          1.9214e-02, -1.8266e-03],\n",
      "        [ 1.0904e-03,  5.0941e-04, -1.7850e-03,  ...,  6.7175e-03,\n",
      "         -5.2803e-03,  5.2586e-04]])\n",
      "episode: 360  mean last ten: 251.0\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[18.4773],\n",
      "        [20.8154],\n",
      "        [19.6000],\n",
      "        [20.9798],\n",
      "        [19.7977],\n",
      "        [20.0883],\n",
      "        [19.3201],\n",
      "        [20.1383],\n",
      "        [18.8287],\n",
      "        [19.7246],\n",
      "        [19.1650],\n",
      "        [19.4191],\n",
      "        [18.8686],\n",
      "        [19.5747],\n",
      "        [18.8301],\n",
      "        [19.6523]], grad_fn=<GatherBackward0>)\n",
      "loss: 1.0877918004989624\n",
      "tensor([[-0.0482, -0.0373,  0.0472,  ..., -0.0533,  0.0234,  0.0119],\n",
      "        [ 0.0039, -0.0005, -0.0009,  ...,  0.0065, -0.0003, -0.0017],\n",
      "        [ 0.0067,  0.0072, -0.0071,  ...,  0.0067, -0.0038, -0.0008],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0005,  0.0007,  ...,  0.0011,  0.0004, -0.0009],\n",
      "        [ 0.0161,  0.0097, -0.0135,  ...,  0.0193, -0.0066, -0.0050],\n",
      "        [-0.1307, -0.0787,  0.1094,  ..., -0.1500,  0.0504,  0.0387]])\n",
      "tensor([19.8437, 19.8493])\n",
      "tensor([14.2514, 16.3118])\n",
      "episode: 370  mean last ten: 212.9\n",
      "episode: 380  mean last ten: 145.0\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[16.6224],\n",
      "        [18.0291],\n",
      "        [17.1737],\n",
      "        [17.8453],\n",
      "        [16.7144],\n",
      "        [18.1299],\n",
      "        [16.7507],\n",
      "        [18.0168],\n",
      "        [17.0867],\n",
      "        [18.0769],\n",
      "        [17.2067],\n",
      "        [18.4739],\n",
      "        [17.6410],\n",
      "        [18.5327],\n",
      "        [18.0192],\n",
      "        [18.8012]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.10716596245765686\n",
      "tensor([[-2.6049e-02, -1.0738e-02,  6.5743e-03,  ..., -2.3214e-02,\n",
      "          3.3481e-02, -1.5143e-03],\n",
      "        [-6.1147e-03, -2.6005e-03,  1.7988e-03,  ..., -5.3399e-03,\n",
      "          7.9240e-03, -3.6915e-04],\n",
      "        [-4.3218e-03, -2.0129e-03,  1.1016e-03,  ..., -4.1818e-03,\n",
      "          6.4431e-03, -2.8222e-04],\n",
      "        ...,\n",
      "        [-4.3937e-02, -2.0086e-02,  1.0676e-02,  ..., -4.1683e-02,\n",
      "          6.2976e-02, -2.8331e-03],\n",
      "        [-8.5518e-03, -4.1018e-03,  2.0422e-03,  ..., -8.2219e-03,\n",
      "          1.2456e-02, -5.9334e-04],\n",
      "        [-8.5455e-04, -4.7651e-04,  1.6101e-04,  ..., -7.8158e-04,\n",
      "          1.0203e-03, -6.2408e-05]])\n",
      "tensor([18.0620, 18.4587])\n",
      "tensor([19.7090, 19.6833])\n",
      "episode: 390  mean last ten: 157.6\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[14.7691],\n",
      "        [16.9397],\n",
      "        [15.9815],\n",
      "        [16.7169],\n",
      "        [16.3275],\n",
      "        [17.6899],\n",
      "        [15.5963],\n",
      "        [17.3743],\n",
      "        [16.6133],\n",
      "        [16.5642],\n",
      "        [15.7166],\n",
      "        [17.3528],\n",
      "        [15.6513],\n",
      "        [15.9348],\n",
      "        [16.0904],\n",
      "        [16.4635]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.11004938185214996\n",
      "tensor([[-2.5215e-02, -6.8929e-03,  4.8171e-03,  ..., -2.4552e-02,\n",
      "          2.3514e-02, -9.4220e-04],\n",
      "        [-6.0604e-03, -1.8155e-03,  1.2718e-03,  ..., -6.4239e-03,\n",
      "          6.2685e-03, -2.7484e-04],\n",
      "        [-8.2239e-04, -1.2734e-04, -3.0769e-04,  ..., -1.7776e-03,\n",
      "          1.6629e-03, -7.9199e-05],\n",
      "        ...,\n",
      "        [-5.4172e-02, -1.4925e-02,  8.8960e-03,  ..., -5.8044e-02,\n",
      "          5.5897e-02, -2.3682e-03],\n",
      "        [-6.8947e-03, -2.0149e-03,  1.1034e-03,  ..., -7.5806e-03,\n",
      "          8.3434e-03, -3.7730e-04],\n",
      "        [-2.0892e-03, -4.5557e-04,  1.4239e-04,  ..., -2.0904e-03,\n",
      "          2.0747e-03, -7.1190e-05]])\n",
      "episode: 400  mean last ten: 179.3\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([11, 1])\n",
      "burned_targets: torch.Size([11, 1])\n",
      "q_values: tensor([[23.6865],\n",
      "        [23.9989],\n",
      "        [23.7909],\n",
      "        [22.7612],\n",
      "        [16.4094],\n",
      "        [16.6293],\n",
      "        [19.9881],\n",
      "        [11.1608],\n",
      "        [13.6053],\n",
      "        [14.6443],\n",
      "        [18.4851]], grad_fn=<GatherBackward0>)\n",
      "loss: 2.9993913173675537\n",
      "tensor([[ 0.0109,  0.0338,  0.0068,  ...,  0.0464, -0.0350, -0.0821],\n",
      "        [ 0.0078,  0.0036,  0.0027,  ...,  0.0116, -0.0099, -0.0192],\n",
      "        [ 0.0558,  0.0880,  0.0093,  ...,  0.1395, -0.1075, -0.2230],\n",
      "        ...,\n",
      "        [ 0.0336,  0.0389, -0.0009,  ...,  0.0630, -0.0500, -0.0939],\n",
      "        [ 0.0671,  0.0873,  0.0005,  ...,  0.1397, -0.1094, -0.2127],\n",
      "        [ 0.0762, -0.0040, -0.0815,  ..., -0.0266,  0.0042,  0.1361]])\n",
      "tensor([23.2529, 22.7469])\n",
      "tensor([23.8156, 23.4615])\n",
      "episode: 410  mean last ten: 244.5\n",
      "reward: tensor([1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([5, 1])\n",
      "burned_targets: torch.Size([5, 1])\n",
      "q_values: tensor([[23.8300],\n",
      "        [23.8787],\n",
      "        [24.0557],\n",
      "        [23.8492],\n",
      "        [22.8851]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.09159888327121735\n",
      "tensor([[-0.0073, -0.0045,  0.0072,  ..., -0.0052,  0.0011, -0.0029],\n",
      "        [ 0.0065,  0.0040, -0.0061,  ...,  0.0045, -0.0010,  0.0025],\n",
      "        [-0.0114, -0.0073,  0.0110,  ..., -0.0086,  0.0018, -0.0050],\n",
      "        ...,\n",
      "        [ 0.0019,  0.0019, -0.0033,  ...,  0.0021, -0.0002,  0.0006],\n",
      "        [ 0.0066,  0.0043, -0.0070,  ...,  0.0048, -0.0010,  0.0024],\n",
      "        [ 0.0091,  0.0059, -0.0088,  ...,  0.0068, -0.0014,  0.0038]])\n",
      "tensor([23.1824, 22.3312])\n",
      "tensor([15.1942, 11.4283])\n",
      "episode: 420  mean last ten: 218.2\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[23.1766],\n",
      "        [15.5675],\n",
      "        [22.6244],\n",
      "        [20.2106],\n",
      "        [22.3565],\n",
      "        [20.0717],\n",
      "        [23.0553],\n",
      "        [19.6457],\n",
      "        [22.1983],\n",
      "        [19.4569],\n",
      "        [22.4479],\n",
      "        [20.2261],\n",
      "        [23.5065],\n",
      "        [20.8503],\n",
      "        [23.5959],\n",
      "        [21.5858]], grad_fn=<GatherBackward0>)\n",
      "loss: 1.0992202758789062\n",
      "tensor([[ 0.0831,  0.0168,  0.0149,  ...,  0.0814, -0.0124,  0.0082],\n",
      "        [ 0.0140,  0.0033,  0.0055,  ...,  0.0204, -0.0036,  0.0053],\n",
      "        [-0.1010, -0.0288,  0.0473,  ..., -0.1645,  0.1186, -0.0970],\n",
      "        ...,\n",
      "        [ 0.1076,  0.0324,  0.0335,  ...,  0.2162, -0.0673,  0.0874],\n",
      "        [ 0.0488,  0.0147,  0.0172,  ...,  0.0945, -0.0300,  0.0378],\n",
      "        [-0.0013, -0.0008,  0.0038,  ..., -0.0046,  0.0080, -0.0034]])\n",
      "episode: 430  mean last ten: 182.5\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[22.3549],\n",
      "        [22.4907],\n",
      "        [23.0666],\n",
      "        [22.8265],\n",
      "        [22.9785],\n",
      "        [22.8680],\n",
      "        [23.2156],\n",
      "        [22.9790],\n",
      "        [23.1756],\n",
      "        [23.0710],\n",
      "        [23.0742],\n",
      "        [23.0548],\n",
      "        [23.1497],\n",
      "        [23.0304],\n",
      "        [22.9902],\n",
      "        [22.9875]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.023051626980304718\n",
      "tensor([[ 0.0006,  0.0004, -0.0004,  ...,  0.0007, -0.0002,  0.0002],\n",
      "        [ 0.0014,  0.0006,  0.0006,  ...,  0.0012, -0.0005,  0.0006],\n",
      "        [-0.0008, -0.0003, -0.0001,  ..., -0.0007,  0.0003, -0.0003],\n",
      "        ...,\n",
      "        [-0.0028, -0.0011, -0.0029,  ..., -0.0031,  0.0009, -0.0013],\n",
      "        [-0.0014, -0.0007, -0.0022,  ..., -0.0019,  0.0005, -0.0007],\n",
      "        [-0.0010, -0.0003, -0.0004,  ..., -0.0008,  0.0003, -0.0004]])\n",
      "tensor([17.2644, 17.3791])\n",
      "tensor([16.7521, 16.4117])\n",
      "episode: 440  mean last ten: 220.6\n",
      "reward: tensor([1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([5, 1])\n",
      "burned_targets: torch.Size([5, 1])\n",
      "q_values: tensor([[22.4821],\n",
      "        [22.4014],\n",
      "        [22.1507],\n",
      "        [21.9451],\n",
      "        [22.2190]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.050172604620456696\n",
      "tensor([[ 8.9765e-03,  3.1222e-03, -4.5395e-03,  ...,  7.2893e-03,\n",
      "         -2.7808e-04,  4.5765e-03],\n",
      "        [ 4.8801e-03,  1.6315e-03, -2.7190e-03,  ...,  3.8157e-03,\n",
      "         -1.3860e-04,  2.4798e-03],\n",
      "        [ 9.2630e-03,  3.1206e-03, -5.2910e-03,  ...,  7.1228e-03,\n",
      "         -2.6793e-04,  4.8078e-03],\n",
      "        ...,\n",
      "        [-3.3113e-02, -1.1385e-02,  1.7634e-02,  ..., -2.6308e-02,\n",
      "          1.0001e-03, -1.7055e-02],\n",
      "        [-1.0866e-02, -3.7698e-03,  5.7057e-03,  ..., -8.6724e-03,\n",
      "          3.3464e-04, -5.6234e-03],\n",
      "        [ 2.8226e-03,  1.0380e-03, -1.3032e-03,  ...,  2.3547e-03,\n",
      "         -9.7947e-05,  1.4973e-03]])\n",
      "tensor([20.0817, 20.0333])\n",
      "tensor([ 7.1557, 10.3957])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[21.4034],\n",
      "        [21.0530],\n",
      "        [21.3829],\n",
      "        [21.1897],\n",
      "        [21.1826],\n",
      "        [21.1388],\n",
      "        [21.1977],\n",
      "        [20.5894],\n",
      "        [20.7999],\n",
      "        [20.2207],\n",
      "        [21.0262],\n",
      "        [19.7655],\n",
      "        [20.0521],\n",
      "        [20.7129],\n",
      "        [21.0091],\n",
      "        [20.9807]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.1858469396829605\n",
      "tensor([[-0.0674, -0.0402,  0.0525,  ..., -0.0264, -0.0037, -0.0155],\n",
      "        [ 0.0280,  0.0148, -0.0223,  ...,  0.0108,  0.0015,  0.0060],\n",
      "        [ 0.0060,  0.0041, -0.0047,  ...,  0.0021,  0.0004,  0.0014],\n",
      "        ...,\n",
      "        [ 0.0316,  0.0202, -0.0255,  ...,  0.0122,  0.0019,  0.0070],\n",
      "        [ 0.0375,  0.0217, -0.0301,  ...,  0.0139,  0.0022,  0.0079],\n",
      "        [ 0.0188,  0.0092, -0.0146,  ...,  0.0074,  0.0009,  0.0041]])\n",
      "episode: 450  mean last ten: 226.2\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "targets: torch.Size([16, 1])\n",
      "burned_targets: torch.Size([16, 1])\n",
      "q_values: tensor([[19.4596],\n",
      "        [19.1700],\n",
      "        [19.9842],\n",
      "        [19.3281],\n",
      "        [19.7305],\n",
      "        [18.1640],\n",
      "        [15.4278],\n",
      "        [12.5633],\n",
      "        [15.0181],\n",
      "        [16.5734],\n",
      "        [18.2409],\n",
      "        [14.5531],\n",
      "        [13.3678],\n",
      "        [16.1895],\n",
      "        [18.4319],\n",
      "        [19.2080]], grad_fn=<GatherBackward0>)\n",
      "loss: 1.8226332664489746\n",
      "tensor([[ 0.0658, -0.0100, -0.0520,  ...,  0.0216, -0.0402,  0.0065],\n",
      "        [-0.0636,  0.0658,  0.0395,  ..., -0.0322,  0.0199, -0.0010],\n",
      "        [ 0.0034,  0.0206, -0.0033,  ...,  0.0020, -0.0062,  0.0036],\n",
      "        ...,\n",
      "        [-0.0690, -0.0142,  0.0443,  ..., -0.0214,  0.0344, -0.0076],\n",
      "        [ 0.1360,  0.1969, -0.0954,  ...,  0.0328, -0.0972,  0.0407],\n",
      "        [ 0.0190,  0.0105, -0.0134,  ...,  0.0059, -0.0113,  0.0027]])\n",
      "tensor([14.0774, 13.5101])\n",
      "tensor([15.6276, 16.2830])\n"
     ]
    }
   ],
   "source": [
    " # Environment parameters\n",
    "ENV = \"CartPole-v1\"\n",
    "env = gym.make('CartPole-v1')\n",
    "# убираем скорость из состояний\n",
    "# env = MaskVelocityWrapper(env)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "episode_n = 1000\n",
    "\n",
    "agent = DRQNAgent(state_dim, action_dim, episode_n=episode_n)\n",
    "\n",
    "total_rewards = []\n",
    "loss1 = []\n",
    "grads1 = []\n",
    "loss2 = []\n",
    "grads2 = []\n",
    "counter = 0\n",
    "for episode in range(episode_n):\n",
    "\n",
    "    total_reward = 0\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        agent.add_sample(state, action, reward, next_state, terminated or truncated)\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 2 == 0:\n",
    "            agent.fit()\n",
    "    \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    \n",
    "        \n",
    "    total_rewards.append(total_reward)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"episode: {episode}  mean last ten: {np.mean(total_rewards[-10:])}\")\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ea6c970-b5b9-43ca-90d9-c26496ba5a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 4]).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9d122ca-0f32-45a9-9a75-2fe49e131fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1)\n",
    "input = torch.randn(1, 10)\n",
    "h0 = torch.randn(1, 20)\n",
    "c0 = torch.randn(1, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70787dd0-9de1-4bd6-b0ad-a8f567fdc42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1831,  0.2089,  0.4142, -0.0557, -0.0669,  0.3513, -0.1570, -0.2058,\n",
       "          0.1245,  0.1779,  0.2874, -0.1077, -0.2030, -0.0157, -0.0557,  0.1683,\n",
       "          0.0930,  0.0799, -0.3804,  0.1875]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "09077e12-3d59-4c1e-8256-ea329000b813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1831,  0.2089,  0.4142, -0.0557, -0.0669,  0.3513, -0.1570, -0.2058,\n",
       "          0.1245,  0.1779,  0.2874, -0.1077, -0.2030, -0.0157, -0.0557,  0.1683,\n",
       "          0.0930,  0.0799, -0.3804,  0.1875]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ff6f1269-dab1-4206-ab7b-d4f10a3d9066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3891,  0.4474,  1.2947, -0.1222, -0.1394,  1.9529, -0.2809, -0.3401,\n",
       "          0.3209,  0.8142,  0.6584, -0.2212, -0.4863, -0.0372, -0.0857,  0.6757,\n",
       "          0.2002,  0.1816, -0.9577,  0.4648]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "865664d4-4ce8-40aa-981e-7b155f531a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
